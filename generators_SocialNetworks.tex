\subsection{Social Networks}
\label{sec:generators_socialnetworks}

On-line social networks, like Facebook, Twitter, or LinkedIn, have become a
phenomenon used by billions of people every day and thus providing extremely
useful information for various domains. However, an analysis of such type of
graph has to cope with two problems: (1) availability of the data and (2)
privacy of the data. Hence, data generators which provide realistic synthetic
social network graphs are in a great demand.

In general, any analysis of social networks identifies their various specific
features~\cite{Chakrabarti:2006:GML:1132952.1132954}. For example, a social
graph often has high \emph{clustering coefficient}, i.e. the degree of transitivity of
a graph. Or, its diameter, i.e. the longest shortest path amongst some fraction (e.g. 90\%) of all connected nodes, is usually low due to weak ties joining faraway cliques.

Another important aspect
of social networks is the community effect. A detailed study of structure of
communities in 70 real-world networks is provided, e.g., in
~\cite{Leskovec:2008:SPC:1367497.1367591}.
~\cite{Prat-Perez:2014:CSS:2621934.2621942} analyzed the structure of
communities (clustering coefficient, triangle participation ratio, bridges,
diameter, conductance and size) in both real-world graphs and outputs of existing graph
generators LFR~\cite{PhysRevE.78.046110} and the
LDBC-SNB~\cite{Erling:2015:LSN:2723372.2742786}. They found out that discovered communities  in different graphs have common distributions and that communities in a single graph have diverse nature and are difficult to fit with a single model.

The existing social network generators try to reproduce different aspects of the
generated network. They can be categorized into statistical and agent-based.
\emph{Statistical
approaches}~\cite{PhysRevE.78.046110,Yao2011,Armstrong:2013:LDB:2463676.2465296,Pham2013,Sukthankar-SocialInfo2014,Erling:2015:LSN:2723372.2742786,Nettleton2016}
focused on reproducing aspects of the network. In \emph{agent-based
approaches}~\cite{Barrett:2009:GAL:1995456.1995598,Bernstein:2013:SAS:2499604.2499609}
the networks are constructed by directly simulating the agents' social choices.

%\paragraph{LFR} Lancichinetti, Fortnato and Radicchi (hence
%LFR)~\cite{PhysRevE.78.046110} develop a class of benchmark graphs whose nodes
%participate in internal community structures. The benchmark models directed and
%weighted real-world networks (e.g., social networks) containing overlapping
%communities of different sizes. The algorithm assumes that both the degree and
%the community size distributions are power laws. Each node shares a fraction $(1
%- \mu)$ of its links with the other nodes of its community and a fraction $\mu$
%with the other nodes of the network, where $\mu$ is called \emph{mixing
%parameter}. The sizes of the communities are taken from a power law distribution
%such that the sum of all sizes equals the number of nodes of the graph. The
%generation process starts with an empty graph and incrementally fills in the
%adjacency matrix by obeying the described constraints.

\paragraph{Realistic Social Network}
~\cite{Barrett:2009:GAL:1995456.1995598} focused on the construction of
realistic social networks. For this purpose the authors combine both public and private data sets
with large-scale agent-based techniques. The process works as follows: In the first step
it generates a synthetic data by combining public and commercial databases. In the second step, it determines a set of activity templates. A 24-hour activity sequence including
geolocations is assigned to each synthetic individual. To demonstrate the approach, the authors create a synthetic US population consisting of people and households together with respective geolocations. For this purpose the authors combine simulation and data fusion techniques utilizing various real-world data sources such as U.S. Census data, responses to an activity survey or a time-use survey. %Demographic information for each person and location, a minute-by-minute schedule of each person's activities, and the locations where these activities take place is generated by a combination of simulation and data fusion techniques.
The result is captured by a dynamic social contact network. Similar methods for agent-based strategies have been reported in~\cite{Bernstein:2013:SAS:2499604.2499609}.

\paragraph{Linkage vs. Activity Graphs} \cite{Yao2011} distinguished between two
types of social network graphs -- the \emph{linkage graph}, where nodes represent people and edges correspond to their friendships, and the \emph{activity graph}, where nodes also represent people but correspond to their interactions. On the basis of the analysis of
Flickr\footnote{\url{https://www.flickr.com/}} social links and
Epinions\footnote{\url{http://www.epinions.com/}} network of user interactions, the authors discover that they both exhibit high clustering coefficient (community structure), small diameter, and power-law degree distribution. Considering the dynamic properties they both have relatively stable clustering coefficient over time and follow the densification law. On the other hand, diameter shrinking is not observed in Epinions activity graph and there is a difference in degree
correlation (i.e., frequency of mutual connections of similar nodes) -- linkage graphs have positive degree correlation, activity graphs have neutral degree correlation. With regards to the findings, the proposed generator focusses on linkage graphs with positive degree correlation. For this purpose it extends the forest fire spreading process algorithm~\cite{Leskovec:2005:GOT:1081870.1081893} with link symmetry. It has two parameters: the \emph{burning probability} $P_b$ and the \emph{symmetry probability} $P_s$. $P_b$ ensures a BFS-based forward burning process in which fire burns strongly with $P_b$ approaching 1.  $P_s$ ensures backward linking from old nodes to new nodes and ``adds fuel to the fire as it brings more links''. %It gives chances for big nodes to connect back to big nodes.


\paragraph{LinkBench} The LinkBench
benchmark~\cite{Armstrong:2013:LDB:2463676.2465296} has been designed for the purpose of analysis of efficiency of a database storing Facebook's production data. The benchmark considers true Big Data and related problems with sharding, replication etc. The social graph at Facebook comprises objects (nodes with IDs, version, timestamp and data) and associations (directed edges, pairs of node IDs, with visibility, timestamp and data). The size of the target graph is the number of nodes. Graph edges and nodes are generated concurrently during bulk loading. The space of node IDs is divided into chunks which enable parallel processing. The edges of the graph are generated in accordance with the results of analysing  real-world Facebook data (such as outdegree distribution). A workload corresponding to 10 graph operations (such as insert object, count the number of associations etc.) and their respective characteristics over the real-world data is generated for the synthetic data.

\paragraph{S3G2} The Scalable Structure-correlated Social Graph Generator (S3G2)~\cite{Pham2013} is a general framework which produces a directed labeled graph whose vertices represent objects having property values. The respective classes determine the structure of the properties. S3G2 does not aim at generating near real-world data, but at generating synthetic graphs with a correlated structure. Hence, the existing data values influence the probability of choosing a certain property value from a pre-defined dictionary, or the probability of connecting two nodes. For example, the degree distribution can be correlated with the properties of a node and thus, e.g., people having many friend relationships typically post more comments and pictures. The data generation process starts with generating a number of nodes with property values generated according to specified property value correlations and then adding respective edges according to specified correlation dimensions. It has multiple phases, each focusing on one correlation dimension. Data is generated in a Map phase corresponding to a pass along one correlation dimension. Then the data are sorted along the correlation dimension in the following Reduce phase. A heuristic observation that ``the probability that two nodes are connected is typically skewed with respect to some similarity between the nodes'' enables to focus only on sliding window of most probable candidates. The core idea of the framework is demonstrated using an example of a social network (consisting of persons and social activities).  The dictionaries for property values are inspired by DBpedia and provided with 20 property value correlations. The edges are generated according to 3 correlation dimensions.


\paragraph{Cloning of Social Networks} ~\cite{Sukthankar-SocialInfo2014}
introduced a synthetic network generator designed for cloning social network
statistics of an existing dataset. The network starts with a small number of
nodes, and new nodes are added until the network reaches the required number. It
has two basic parameters: homophily and link density. A high \emph{homophily}
value reflects that links have higher chances to be established among the nodes with the
same label; these labels can be considered as being equivalent to community
membership.

Attribute Synthetic Generator (ASG) is a network generator for reproducing the
node feature distribution of standard networks and rewiring the network to
preferentially connect nodes that exhibit a high feature similarity. The network
is initialized with a group of three nodes, and new nodes and links are added to
the network based on link density, homophily, and feature similarity. As new
nodes are created, their labels are assigned based on the prior label
distribution. After the network has reached the same number of nodes as the
original social media dataset, each node initially receives a random attribute
assignment. Then a stochastic optimization process is used to move the initial
assignments closer to the target distribution extracted from social media
dataset using the Particle Swarm Optimization algorithm. The tuned attributes
are then used to add additional links to the network based on the feature
similarity parameter -- a source node is selected randomly and connected to the
most similar node. Multi-Link Generator (MLG) further  uses link co-occurrence statistics from the
original dataset to create a multiplex network. MLG uses the same network growth
process as ASG. Based on the link density parameter, either a new node is
generated with a label based on the label distribution of the target dataset or
a new link is created between two existing nodes.


\paragraph{LDBC SNB} Despite having a common Facebook-like dataset, thanks to three distinct workloads the Social Network Benchmark (SNB)~\cite{Erling:2015:LSN:2723372.2742786} provided by LDBC represents three distinct benchmarks. The network nodes correspond to people and the edges represent their friendship and messages they post in discussion trees on their forums. The three query workloads involve: (1) SNB-Interactive, i.e., complex read-only queries accessing a significant portion of data, (2) SNB-BI, i.e., queries accessing a high percentage of  entities and grouping them in various dimensions, and (3) SNB-Algorithms, involving graph analysis algorithms, such as community detection, PageRank, BFS, and clustering. The graph generator realizes power laws, uses skewed value distributions, and ensures reasonable correlations between graph structures and property values. To provide scalability it is implemented on top of Hadoop.

%The generated data have become a part if several graph benchmarks, such as GraphBIG~\cite{Nai:2015:GUG:2807591.2807626}.

\paragraph{Towards More Realistic Data} \cite{Nettleton2016} argued that the majority of existing works focuses on
topology generation which approximates the characteristics of a real social
network (such as a small graph diameter, skew degree
distribution, small average path length, and community structures), however,  this is usually done without any data. Hence, they
introduced a general stochastic modeling approach that enables the users to
populate a graph topology with data. The approach has three steps: (1) topology
generation (using R-MAT) plus community identification using the Louvain
method~\cite{1742-5468-2008-10-P10008} or usage of a real-world topology from
SNAP\footnote{\url{https://snap.stanford.edu/data/}}, (2) data definition
that describes distribution profiles, attribute value definitions, using a
parameterizable set of data propagation rules and affinities, and (3) data
population.


