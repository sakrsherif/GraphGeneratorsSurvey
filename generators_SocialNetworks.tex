\subsection{Social Networks}
\label{sec:generators_socialnetworks}

On-line social networks, like Facebook, Twitter, or LinkedIn, have become a
phenomenon used by billions of people every day and thus providing extremely
useful information for various domains. However, an analysis of such type of
graph has to cope with two problems: (1) availability of the data and (2)
privacy of the data. Hence, data generators which provide realistic synthetic
social network graphs are in a great demand.

In general, any analysis of social networks identifies their various specific
features~\cite{Chakrabarti:2006:GML:1132952.1132954}. For example, a social
graph often has high \emph{clustering coefficient}, i.e. the degree of transitivity of
a graph. Or, its diameter, i.e. the longest shortest path amongst some fraction (e.g. 90\%) of all connected nodes, is usually low due to weak ties joining faraway cliques.

Another important aspect
of social networks is the community effect. A detailed study of structure of
communities in 70 real-world networks is provided, e.g., in
~\cite{Leskovec:2008:SPC:1367497.1367591}.
~\cite{Prat-Perez:2014:CSS:2621934.2621942} analyzed the structure of
communities (clustering coefficient, triangle participation ratio, bridges,
diameter, conductance and size) in both real-world graphs and outputs of existing graph
generators LFR~\cite{PhysRevE.78.046110} and the
LDBC-SNB~\cite{Erling:2015:LSN:2723372.2742786}. They discover that communities found in different graphs follow quite similar distributions and that communities in a single graph have diverse nature and are difficult to fit with a single model.

The existing social network generators try to reproduce different aspects of the
generated network. They can be categorized into statistical and agent-based.
\emph{Statistical
approaches}~\cite{PhysRevE.78.046110,Yao2011,Armstrong:2013:LDB:2463676.2465296,Pham2013,Sukthankar-SocialInfo2014,Erling:2015:LSN:2723372.2742786,Nettleton2016}
focused on reproducing aspects of the network. In \emph{agent-based
approaches}~\cite{Barrett:2009:GAL:1995456.1995598,Bernstein:2013:SAS:2499604.2499609}
the networks are constructed by directly simulating the agents' social choices.

%\paragraph{LFR} Lancichinetti, Fortnato and Radicchi (hence
%LFR)~\cite{PhysRevE.78.046110} develop a class of benchmark graphs whose nodes
%participate in internal community structures. The benchmark models directed and
%weighted real-world networks (e.g., social networks) containing overlapping
%communities of different sizes. The algorithm assumes that both the degree and
%the community size distributions are power laws. Each node shares a fraction $(1
%- \mu)$ of its links with the other nodes of its community and a fraction $\mu$
%with the other nodes of the network, where $\mu$ is called \emph{mixing
%parameter}. The sizes of the communities are taken from a power law distribution
%such that the sum of all sizes equals the number of nodes of the graph. The
%generation process starts with an empty graph and incrementally fills in the
%adjacency matrix by obeying the described constraints.

\paragraph{Realistic Social Network}
~\cite{Barrett:2009:GAL:1995456.1995598} focused on the construction of
realistic social networks using a combination of public and private data sets
and large-scale agent based techniques. The process works as follows: In the first step
it creates a synthetic population by integrating databases from commercial and
public sources. In the second step, a set of activity templates are determined. Each
synthetic individual is assigned a 24-hour activity sequence including
geolocations for each activity. To demonstrate the approach, the authors develop a synthetic population for the
United States that models every individual in the population. The synthetic
population is a set of geographically located people and households. Household
structure and demographics are derived from U.S. Census data. The activity
templates are  based on several thousand responses to an activity or time-use
survey. Demographic information for each person and location, a minute-by-minute
schedule of each person's activities, and the locations where these activities
take place is generated by a combination of simulation and data fusion
techniques. This information is captured by a dynamic social contact network. Similar methods for agent-based strategies have been reported in~\cite{Bernstein:2013:SAS:2499604.2499609}.

\paragraph{Linkage vs. Activity Graphs} \cite{Yao2011} distinguished between two
types of social network graphs -- the \emph{linkage graph}, where nodes stand
for the people in the social network and edges are their friendship links, and
the \emph{activity graph}, where nodes also stand for the people but edges stand
for their interactions. On the basis of the analysis of
Flickr\footnote{\url{https://www.flickr.com/}} social links and
Epinions\footnote{\url{http://www.epinions.com/}} network of user interactions,
the authors discover that they both exhibit power-law degree distribution, high
clustering coefficient (community structure), and small diameter; also regarding
the dynamic properties they both follow the densification law and have relatively stable
clustering coefficient over time. However, the authors do not observe diameter
shrinking in opinions activity graph and there is a difference in degree
correlation (how frequently nodes with similar degrees connect to each other).
Namely linkage graphs have positive degree correlation whereas activity graphs
show neutral degree correlation. With regards to the findings, the proposed generator focusses on linkage graphs
with positive degree correlation. For this purpose it extends the forest
fire spreading process algorithm~\cite{Leskovec:2005:GOT:1081870.1081893} with
link symmetry. It has two parameters -- the \emph{burning probability} $P_b$
which is in charge of the burning process, and the \emph{symmetry probability}
$P_s$ which indicates backward linking from old nodes to new ones. $P_b$
controls a BFS-based forward burning process. The fire burns increasingly
fiercely with $P_b$ approaching 1. Meanwhile, $P_s$ adds fuel to the fire as it
brings more links. It gives chances for big nodes to connect back to big nodes.


\paragraph{LinkBench} The LinkBench
benchmark~\cite{Armstrong:2013:LDB:2463676.2465296} has been designed to predict the
performance of a database when used for persistent storage of Facebook's
production data. The benchmark considers true Big Data and related problems with
sharding, replication etc. The social graph at Facebook comprises objects (nodes
with IDs, version, timestamp and data) and associations (directed edges, pairs
of node IDs, with visibility, timestamp and data). The size of the target graph
is the number of nodes. Graph edges are generated concurrently with graph nodes
during bulk loading. The node ID space is divided into chunks based on the ID of
the source node which  are processed in parallel. The edges of the graph are
generated in accordance with the results of analysing  real-world Facebook data
(such as outdegree distribution). A workload corresponding to 10 graph
operations (such as insert object, count the number of associations etc.) and
their respective characteristics over the real-world data is generated for the
synthetic data.

\paragraph{S3G2} The Scalable Structure-correlated Social Graph Generator
(S3G2)~\cite{Pham2013} is a general framework which  generates a directed
labeled graph, where the nodes are objects with property values, and their
structure is determined by the class a node belongs to. S3G2 does not aim at
generating near real-world data, but at generating synthetic graphs with a
correlated structure. It causes that the probability to choose a certain
property value (from a pre-defined dictionary), or the probability to connect
two nodes with an edge are influenced by existing data values. For example, it
is possible to have a correlated degree distribution, from which the degree of
each node is generated, correlated with the properties of node. Hence the generator
can ensure that, e.g., people with many friends in a social network will
typically post more pictures than people with few friends, i.e., the amount of
friend nodes influences the amount of posted comment and picture nodes. The data generation process starts with generating a number of nodes with
property values generated according to specified property value correlations and
then adding respective edges according to specified correlation dimensions. It
has multiple phases, each focusing on one correlation dimension. Each pass along
one correlation dimension is a Map phase in which data is generated, followed by
a Reduce phase that sorts the data along the correlation dimension in the next
pass. A heuristic observation that ``the probability that two nodes are
connected is typically skewed with respect to some similarity between the
nodes'' enables to focus only on sliding window of most probable candidates. The core idea of the framework is demonstrated using an example of a social
network (consisting of persons and social activities).  The dictionaries for
property values are inspired by DBpedia and provided with 20 property value
correlations. The edges are generated according to 3 correlation dimensions.


\paragraph{Cloning of Social Networks} Paper~\cite{Sukthankar-SocialInfo2014}
introduces a synthetic network generator designed for cloning social network
statistics of an existing dataset. The network starts with a small number of
nodes, and new nodes are added until the network reaches the required number. It
has two basic parameters: homophily and link density. A high \emph{homophily}
value indicates that links are more likely to be formed between nodes with the
same label; these labels can be viewed as being equivalent to community
membership.

Attribute Synthetic Generator (ASG) is a network generator for reproducing the
node feature distribution of standard networks and rewiring the network to
preferentially connect nodes that exhibit a high feature similarity. The network
is initialized with a group of three nodes, and new nodes and links are added to
the network based on link density, homophily, and feature similarity. As new
nodes are created, their labels are assigned based on the prior label
distribution. After the network has reached the same number of nodes as the
original social media dataset, each node initially receives a random attribute
assignment. Then a stochastic optimization process is used to move the initial
assignments closer to the target distribution extracted from social media
dataset using the Particle Swarm Optimization algorithm. The tuned attributes
are then used to add additional links to the network based on the feature
similarity parameter -- a source node is selected randomly and connected to the
most similar node. Multi-Link Generator (MLG) further  uses link co-occurrence statistics from the
original dataset to create a multiplex network. MLG uses the same network growth
process as ASG. Based on the link density parameter, either a new node is
generated with a label based on the label distribution of the target dataset or
a new link is created between two existing nodes.


\paragraph{LDBC SNB} The Social Network Benchmark
(SNB)~\cite{Erling:2015:LSN:2723372.2742786} provided by LDBC consists of three
distinct benchmarks on a common dataset corresponding to three different
workloads. SNB models a social network akin to Facebook. The dataset consists of
persons and a friendship network that connects them; whereas the majority of the
data is in the messages that these persons post in discussion trees on their
forums. The three query workloads involve: (1) SNB-Interactive, i.e., complex
read-only queries, that touch a significant amount of data, (2) SNB-BI which
accesses a large percentage of all entities in the dataset and groups these in
various dimensions, and (3) SNB-Algorithms, i.e., graph analysis algorithms,
including PageRank, Community Detection, Clustering and Breadth First Search.
The graph generator realizes power laws, uses skewed value distributions, and
introduces plausible correlations between property values and graph structures.
It is implemented on top of Hadoop to provide scalability.

%The generated data have become a part if several graph benchmarks, such as GraphBIG~\cite{Nai:2015:GUG:2807591.2807626}.

\paragraph{Towards More Realistic Data} \cite{Nettleton2016} argued that the main body of existing work lies in
topology generation which approximates the characteristics of a real social
network (such as a small graph diameter, small average path length, skew degree
distribution, and community structures), however,  this is usually done without any data. Hence, they
introduced a general stochastic modeling system which allows the users to
populate a graph topology with data. The approach has three steps: (1) topology
generation (using R-MAT) plus community identification using the Louvain
method~\cite{1742-5468-2008-10-P10008} or usage of a real-world topology from
SNAP\footnote{\url{https://snap.stanford.edu/data/}}, (2) data definition
following distribution profiles, attribute value definitions, using a
parameterizable set of data propagation rules and affinities, and (3) data
population.


