\subsection{Semantic Web}
\label{sec:generators_LinkedData}
With the dawn of the concept of Linked Data it is a natural development that there would emerge respective benchmarks involving both synthetic data and real-world data sets  sets with real-world characteristics. The used data sets correspond to RDF representation of relational-like data~\cite{Guo2005158,Bizer09theberlin}, social network-like data~\cite{Schmidt2010}, or specific and significantly more complex data structures such as biological data~\cite{Wu2014}. In this section, we provide an overview of benchmarking systems involving a kind of graph-based RDF data generator or data modifier. %Other types of systems or particular results can be found, e.g., at~\cite{RdfStoreBenchmarking}.

\iffalse
Considering the Big Data world, the Linked Data in general definitely belong to this group since we assume that the Linked (Open) Data Sets form a common Linked Open Data cloud\footnote{\url{http://lod-cloud.net/}}. On the other hand, the particular data sets can be relatively small.
\fi

\paragraph{LUBM} The use-case driven Lehigh University Benchmark (LUBM)\footnote{\url{http://swat.cse.lehigh.edu/projects/lubm/}} considers the university domain. The ontology defines 43 classes and 32 properties~\cite{Guo2005158}. In addition, the LUBM benchmark provides 14 test queries. In particular, the benchmark focuses on \emph{extensional} queries, i.e., queries which target the instance data over ontologies, as an opposite to \emph{intentional} queries, i.e., queries which target the properties and  classes of the ontology. The Univ-Bench Artificial  (UBA) data generator features repeatable and random  data generation (exploiting classical linear congruential generator, LCG, of numbers). In particular, the data which is produced by the generator are assigned zero-based indexes (i.e., the first university is named \emph{University0} and so on), thus they are reproducible at any time with the same indexes.  The generator allows to specify a seed for random number generation, along with the desired number of universities, and the starting index of the universities.

An extension of LUBM, the Lehigh BibTeX Benchmark (LBBM)~\cite{Wang2005}, enables generating synthetic data for different ontologies. The data generation process is managed through two main phases: (1) the property-discovery phase, and (2) the data generation phase. In particular, LBBM provides a probabilistic model that can emulate the properties of the data of a particular domain and generate synthetic data exhibiting similar properties. A Monte Carlo algorithm is employed to output synthetic data. The approach is demonstrated on the Lehigh University BibTeX ontology which consists of 28 classes along with 80 properties. The LUBM benchmark includes 12 test queries that were designed for the benchmark data. Another extension of LUBM, the University Ontology Benchmark (UOBM)\footnote{\url{https://www.cs.ox.ac.uk/isg/tools/UOBMGenerator/}}, focuses on two aspects: (1) usage of all constructs of OWL Lite and OWL DL~\cite{owl} and (2) lack of necessary links between the generated data which thus form isolated graphs~\cite{Ma:2006:TCO:2094613.2094629}. In the former case the original ontology is replaced by the two types of extended versions from which the user can choose. In the latter case cross-university and cross-department links are added to create a more complex graph.

\paragraph{IIMB} Contrary to the previous work, Ferrara et al.~\cite{Ferrara08OM} proposed the ISLab Instance Matching Benchmark (IIMB)\footnote{\url{http://www.ics.forth.gr/isl/BenchmarksTutorial/}} for the problem of instance matching. For any two objects $o_1$ and $o_2$ adhering to the same ontology or to different ontologies, instance matching is specified in the form of a function $Om(o_1, o_2) \rightarrow \{0, 1\}$,  where $o_1$ and $o_2$ are linked to the same real-world object (in which case the function maps to $1$) or $o_1$ and $o_2$ are representing different objects (in which case the function maps to $0$). It targets the domain of movie data which contains 15 named classes, along with 5 object properties and 13 datatype properties. The data are extracted from IMDb\footnote{\url{http://www.imdb.com/}}. The data generator corresponds to a data modifier which simulates differences between the data. In particular it involves data value differences (such as typographical errors or usage of different standard formats, e.g., for names), structural heterogeneity (represented by different levels of depth for properties, diverse aggregation criteria for properties, or missing values specification) and logical heterogeneity (such as instantiation on different subclasses of the same superclass or instantiation on disjoint classes).


\paragraph{BSBM} The Berlin SPARQL Benchmark (BSBM)\footnote{\url{http://wifo5-03.informatik.uni-mannheim.de/bizer/berlinsparqlbenchmark/}}, is centered around an e-commerce application domain with object types such as \emph{Customer}, \emph{Vendor}, \emph{Product} and \emph{Offer} in addition to the relationship among them~\cite{Bizer09theberlin}.
The benchmark provides a workload that have 12 queries with 2 types of query workloads (i.e., 2 sequences of the 12 queries) emulating the navigation pattern and search of a consumer seeking a product. The data generator is capable of producing arbitrarily scalable datasets by controlling the number of products ($n$) as a scale factor.  The scale factor also impacts other data characteristics, such as, e.g., the depth of type hierarchy of products (defined as $d = round(log_{10}(n))/2 + 1$), branching factor ($bfr = 2 \times round(log_{10}(n))$), the number of product features (having $lowerBound = 35 \times i / (d \times (d+1)/2 - 1)$ and $upperBound = 75 \times i / (d \times (d+1)/2 - 1)$) etc. BSBM can output two representations, i.e. an RDF representation along with a relational representation. Thus, BSBM also defines an SQL~\cite{sql} representation of the queries. This allows comparison of SPARQL~\cite{sparql} results  to be compared against the performance of traditional RDBMSs.


\paragraph{SP$^2$Bench} The SP$^2$Bench\footnote{\url{http://dbis.informatik.uni-freiburg.de/forschung/projekte/SP2B/}} is a language-specific benchmark~\cite{Schmidt2010} which is based on the DBLP dataset. %, so the types involve Person, Inproceedings, Article etc.
The generated datasets follow the key characteristics of the original DBLP dataset. In particular, the data mimics the correlations between entities. All random functions of the generator use a fixed seed that ensures the data generation process is deterministic. SP$^2$Bench is accompanied by 12 queries covering the various types of operators such as RDF access paths in addition to typical RDF constructs.

%\paragraph{JustBench} ~\cite{Bail:2010:JFO:1940281.1940285} ...

\paragraph{Data Coherence}  A comparison of 4 RDF benchmarks (namely TPC-H~\cite{TPC-H} data expressed in RDF, LUBM, BSBM, and SP$^2$Bench) and 6 real-wold data sets (such as, e.g.,  DBpedia, the Barton Libraries Dataset~\cite{barton-benchmark} or
WordNet~\cite{Miller:1995:WLD:219717.219748}) has been reported by~\cite{Duan:2011:AOC:1989323.1989340}. The authors focus mainly on the  \emph{structuredness} (\emph{coherence}) of each benchmark dataset claiming that a primitive metric (e.g., the number of triples or the average in/outdegree) quantifies only some target characteristics of each dataset. The degree of structuredness of a dataset $D$ with respect to a type $T$ is based on  the regularity of instance data in $D$ in conforming to type $T$. The type system is extracted from the data set by finding the RDF triples that have property  \texttt{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type} and extract type $T$ from their object. Properties of $T$ are determined as the union of all the properties that the instances of type $T$ have. The structuredness is then expressed as a weighted sum of share of set properties of each type, whereas higher weights are assigned to types with more instances. The authors show that the structuredness of the chosen benchmarks is fixed, whereas real-world RDF datasets are belonging to the non-tested area of the spectrum. As a consequence, they introduce a new benchmark that receives as input any dataset associated with a required level of structuredness and size (smaller than the size of the original data), and exploits the input documents as a seed to produce a subset of the original data with the target structuredness and size. In addition, they show that structuredness and size mutually influence each other.


\paragraph{DBPSB} DBpedia SPARQL Benchmark (DBPSB)\footnote{\url{http://aksw.org/Projects/DBPSB.html}} proposed at the University of Leipzig has been designed using workloads that have been issued by humans and applications over existing RDF data~\cite{Morsey2011,Morsey:2012:UBR:2900929.2901031}. In addition, the authors argue that benchmarks like LUBM, BSBM, or SP$^2$Bench resemble relational database benchmarks involving relational data structures with few and homogeneously structured classes, whereas, in reality, RDF datasets are increasingly heterogeneous. For example, DBpedia version 3.6 consists of 289,016 classes of which 275 classes are defined based on the DBpedia ontology. In addition, different datatypes and object references of various types are
used in property values. Hence, they presented a universal SPARQL benchmark generation approach which uses a flexible data production mechanism that mimics the input data source. This dataset generation process begins using an input dataset then multiple datasets with different sizes  are then generated by duplicating all the RDF triples with changing their namespaces.  For generating smaller datasets, an appropriate fraction of all triples is selected randomly or by sampling across classes in the dataset. \iffalse The goal of the query analysis and clustering is to detect prototypical queries on the basis of their frequent usage and similarity.\fi The methodology is applied on the DBpedia SPARQL endpoint and a set of 25 SPARQL query templates is derived, that cover the most commonly used SPARQL features.

\paragraph{LODIB} The Linked Open Data Integration Benchmark (LODIB)\footnote{\url{http://lodib.wbsg.de/}} has been designed with the aim of reflecting the real-world heterogeneities that exist on the Web of Data in order to enable testing of Linked Data translation systems~\cite{DBLP:conf/www/RiveroSBR12}. It provides a catalogue of 15 data translation patterns (e.g., rename class, remove language tag etc.), each of which is a common data translation problem in the context of Linked Data. The benchmark provides a data generator that produces three different synthetic data sets that need to be translated
by the system under test into a single target vocabulary. They  reflect the pattern distribution in analyzed 84 data translation examples from the LOD Cloud. The data sets reflect the same e-commerce scenario used for BSBM.


\paragraph{SIB} The developers of the Social Network Intelligence BenchMark (SIB)\footnote{\url{https://www.w3.org/wiki/Social_Network_Intelligence_BenchMark}} based the design of their benchmark based on the claim that existing benchmarks are limited in reflecting the characteristics of the real RDF datasets and are mostly relational-like. Hence, they proposed a benchmark for  query processing over real graphs~\cite{sib}. The proposed benchmark simulates an RDF backend of a social network site where users and their interactions form a social graph of social activities such as creating/managing groups, writing posts and posting comments. The distribution of generated data on each relation follows the data distribution inferred from real-world social networks. In addition, association rules are included in order to convey the real-world data correlation into synthetic data. The  generated data is linked with the RDF datasets from DBpedia. The benchmark specification contains 3 query mixes -- interactive, update, and analysis -- expressed in SPARQL 1.1 Working Draft.

\paragraph{Geographica} The Geographica benchmark\footnote{\url{http://geographica.di.uoa.gr/}} has been designed to target the area of geospatial data~\cite{DBLP:conf/semweb/GarbisKK13} and respective SPARQL extensions GeoSPARQL~\cite{battle2012enabling} and stSPARQL~\cite{koubarakis2010modeling}. The benchmark involves a real-world workload based on openly available datasets that cover a range of geometry types (e.g., points, lines, polygons) and  a synthetic workload. In the former case there is a (1) a micro benchmark that tests primitive spatial functions (involving 29 queries) and (2) macro benchmark that tests the performance of RDF stores in typical application scenarios like reverse geocoding or map search and browsing (consisting of 11 queries). In the latter case of a synthetic workload the generator produces synthetic datasets of various sizes that corresponds to an ontology based on OpenStreetMap (i.e., states in a country, land ownership, roads and  points of interest) and instantiates query templates. The spatial extent of the land ownership dataset constitutes a uniform grid of $n \times n$ hexagons, whereas the size of each dataset is given relatively to $n$. The synthetic workload generator produces SPARQL queries corresponding to spatial selection and spatial joins by instantiating 2 query templates.


\paragraph{WatDiv} The Waterloo SPARQL Diversity Test Suite (WatDiv)\footnote{\url{http://dsg.uwaterloo.ca/watdiv/}} developed at the University of Waterloo provides stress testing tools to address the observation that existing SPARQL benchmarks are not suitable for testing systems for diverse queries and varied workloads~\cite{Aluc:2014:DST:2717213.2717229}. The benchmark introduces two classes of query features -- structural and data-driven -- and performs a detailed analysis on existing SPARQL benchmarks (LUBM, BSBM, SP$^2$Bench, and DBPSB) using the two classes of query features. The structural features involve triple pattern count, join vertex count, join vertex degree, and join vertex count. The data-driven features involve result cardinality and several types of selectivity. The analysis of the four benchmarks reveals that they are not sufficiently diverse to evaluate the strengths and weaknesses of the various physical design alternatives that have been implemented by the different RDF systems. The proposed solution, WatDiv, involves (1) a data generator which generates scalable datasets according to the WatDiv schema, (2) a query template generator which traverses the WatDiv schema and generates a diverse set of query templates, (3) a query generator which instantiates the templates with actual RDF terms from the dataset, and (4) a feature extractor which extracts the structural features of the generated data and workload. %For the study in the paper the authors generated 12,500 test queries from 125 query templates.

\paragraph{RBench} RBench~\cite{Qiao:2015:RAR:2723372.2746479} is an application-specific benchmark which receives any RDF dataset as an input and produces as an output a set of datasets, that have similar characteristics of the input dataset, using size scaling factor $s$ and (node) degree scaling factor $d$. \iffalse A generated benchmark dataset is considered similar to the given dataset if their values for the dataset evaluation metrics and query evaluation times for different techniques are similar. Three evaluation metrics are utilized for this purpose: dataset coherence (i.e., a measure how uniformly predicates are distributed among the same type/class), relationship specialty (i.e., the number of occurrences of the same predicate associated with each resource), and literal diversity.\fi A query generation process has been implemented to produce 5 different types of queries (edge-based queries, node-based queries, path queries, star queries, subgraph queries) for any generated data. The benchmark project FEASIBLE~\cite{Saleem2015} is also an application-specific benchmark; however, contrary to RBench, it is designed to produce benchmarks from a set of queries (in particular from query logs) by relying on sample queries of a user-defined
size from the input set of queries.


\paragraph{LDBC}  The Linked Data Benchmark Council\footnote{\url{http://ldbcouncil.org/industry/organization/origins}} (LDBC)~\cite{Angles:2014:LDB:2627692.2627697} is a result of a (closed) EU project that brought together a community of academic researchers and industry that had the main objective of developing an open source, yet industrial grade benchmarks for graph and RDF databases. \iffalse The following three benchmarks were developed and are currently maintained.\fi In the Semantic Web domain, the project released the Semantic Publishing Benchmark (SPB)~\cite{spb} that has been inspired by the Media/Publishing industry (namely BBC\footnote{\url{http://www.bbc.com/}}). The application scenario of this benchmark simulates a media or a publishing organization that handles large amount of streaming content (e.g., news, articles). \iffalse This content is enriched with metadata that describes it and links it to reference knowledge -- taxonomies and databases that include relevant concepts, entities and factual information. The SPB data generator produces scalable in size synthetic large data. Synthetic data consists of a large number of annotations of media assets that refer entities found in reference datasets.\fi The data generator mimics three types of relations in the generated synthetic data: clustering of data, correlations of entities, and random tagging of entities. Two workloads are provided: (1) basic, involving an interactive query-mix querying the relations between entities in reference data, and (2) advanced,  focusing on interactive and analytical query-mixes. The LDBC has designed two other benchmarks: the Social Network Benchmark (SNB)~\cite{Erling:2015:LSN:2723372.2742786} for the social network domain  (see Section~\ref{sec:generators_socialnetworks}) and Graphalytics~\cite{Iosup:2016:LGB:3007263.3007270}   for the analytics domain.% (see Section~\ref{sec:generators_analytics}).



\paragraph{LinkGen} LinkGen is a synthetic linked data generator that has been designed to generate RDF datasets for a given vocabulary~\cite{10.1007/978-3-319-46547-0_12}. The generator is designed to receive a vocabulary as an input  and supports two statistical distributions for generating entities: Gaussian distribution and Zipf's power-law distribution. LinkGen can augment the generated data with inconsistent and noisy  data such as writing two conflicting values for a given datatype property,  adding triples with syntactic errors, adding wrong statements by assigning them with invalid domain and creating instances with no type information. The generator is also designed with  an option to inter-link the generated instances with real ones given that the user provides entities from real datasets. The datasets can be generated in any of of two modes: on-disk and streaming.

