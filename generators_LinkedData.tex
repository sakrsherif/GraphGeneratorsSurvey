\subsection{Semantic Web}
\label{sec:generators_LinkedData}
With the dawn of the concept of Linked Data there have naturally emerged also respective benchmarks involving both real-world data sets and synthetic data sets with real-world characteristics. The used data sets correspond to RDF representation of relational-like data~\cite{Guo2005158,Bizer09theberlin}, social network-like data~\cite{Schmidt2010}, or specific and significantly more complex data structures such as biological data~\cite{Wu2014}. In the following overview we focus on benchmarking systems involving a kind of data generator or data modifier. Other types of systems or particular results can be found, e.g., at~\cite{RdfStoreBenchmarking}.

Considering the Big Data world, the Linked Data in general definitely belong to this group since we assume that the Linked (Open) Data Sets form a common Linked Open Data cloud\footnote{\url{http://lod-cloud.net/}}. On the other hand, the particular data sets can be relatively small.

\paragraph{LUBM} The use-case driven Lehigh University Benchmark (LUBM)~\cite{Guo2005158} considers the university domain. The ontology defines 43 classes and 32 properties, including 25 object properties and 7 datatype properties. The LUBM benchmark also provides 14 test queries. The authors focus on \emph{extensional} queries, i.e., queries about the instance data over ontologies, as an opposite to \emph{intentional} queries (i.e., queries about classes and properties). The Univ-Bench Artificial  (UBA) data generator features random and repeatable data generation (exploting classical linear congruential generator, LCG, of numbers). In particular, data generated by the tool are exactly repeatable with respect to universities (assigning them zero-based indexes, i.e., the first university is named University0 and so on).  The generator allows a user to specify the seed for random number generation, the number of universities, and the starting index of the universities.

An extension of LUBM, the Lehigh BibTeX Benchmark (LBBM)~\cite{Wang2005}, enables generating synthetic data for different ontologies. The generation is divided into two phases: (1) the property-discovering phase, and (2) the data generation phase. The authors present a probabilistic model that, given representative data of some domain, can capture the properties of the data and generate synthetic data that has similar properties. A Monte Carlo algorithm is used to generate synthetic data. The approach is demonstrated on the Lehigh University BibTeX ontology which consists of 28 classes and 80 properties. 12 test queries were designed for  the benchmark data.

Another extension of LUBM, the University Ontology Benchmark (UOBM)~\cite{Ma:2006:TCO:2094613.2094629}, focuses on two aspects: (1) usage of all constructs of OWL Lite and OWL DL~\cite{owl} and (2) lack of necessary links between the generated data which thus form isolated graphs. In the former case the original ontology is replaced by the two types of extended versions from which the user can choose. In the latter case cross-university and cross-department links are added to create a more complex graph.

\paragraph{IIMB} Contrary to the previous work, paper~\cite{Ferrara08OM} proposes the ISLab Instance Matching Benchmark (IIMB) for the problem of instance matching defined as follows: Given two instances $i_1$ and $i_2$, belonging to the same ontology or to different ontologies, instance matching is defined as a function $Im(i_1, i_2) \rightarrow \{0, 1\}$,  where $1$ denotes the fact that $i_1$ and $i_2$ are referred to the same real-world object and $0$ denotes the fact that $i_1$ and $i_2$ are referred to different objects. It targets the domain of movie data which contains 15 named classes, 5 object properties and 13 datatype properties. The data are extracted from IMDb\footnote{\url{http://www.imdb.com/}}.

The data generator corresponds to a data modifier which simulates differences between the data. In particular it involves data value differences (such as typographical errors or usage of different standard formats, e.g., for names), structural heterogeneity (represented by different levels of depth for properties representation, different aggregation criteria for properties representation, or missing values specification) and logical heterogeneity (such as instantiation on different subclasses of the same superclass or instantiation on disjoint classes).


\paragraph{BSBM} The Berlin SPARQL Benchmark (BSBM)~\cite{Bizer09theberlin}, also use-case driven, involves an e-commerce domain, where types Product, Offer and Vendor are used to model the relationships between products and the vendors offering them, while types Person and Review are used to model the relationship between users and product reviews these users write.
The benchmark also comes with 12 queries and 2 query mixes (sequences of the 12 queries) emulating the search and navigation pattern of a consumer looking for a product. The data generator supports creation of arbitrarily large datasets using the number of products $n$ as scale factor which further influences also other characteristics of the data, such as, e.g., the depth of type hierarchy of products (defined as $d = round(log_{10}(n))/2 + 1$), branching factor ($bfr = 2 \times round(log_{10}(n))$), the number of product features (having $lowerBound = 35 \times i / (d \times (d+1)/2 ? 1)$ and $upperBound = 75 \times i / (d \times (d+1)/2 ? 1)$) etc.

BSBM can output two representations -- an RDF representation and a purely relational representation. Thus BSBM also defines an SQL~\cite{sql} representation of the queries. This allows comparison of SPARQL~\cite{sparql} results with the performance of traditional RDBMSs.


\paragraph{SP$^2$Bench} The SP$^2$Bench is a language-specific benchmark~\cite{Schmidt2010} which uses the DBLP~\cite{dblp} as a domain for the dataset. %, so the types involve Person, Inproceedings, Article etc.
The generated documents mirror key characteristics and distributions found in the original DBLP database. The data mimics natural correlations between entities, such as power law distributions (found in the citation system or the distribution of publications among authors) and limited growth curves (e.g., the increasing number of venues and publications over time). All random functions of the generator base on a fixed seed which makes data generation deterministic. SP$^2$Bench is accompanied by 12 queries covering different operator constellations, RDF access paths, typical RDF constructs etc.

%\paragraph{JustBench} ~\cite{Bail:2010:JFO:1940281.1940285} ...

\paragraph{Data Coherence}  A comparison of 4 RDF benchmarks (namely TPC-H~\cite{TPC-H} data expressed in RDF, LUBM, BSBM, and SP$^2$Bench) and 6 real-wold data sets (such as, e.g.,  DBpedia~\cite{Bizer:2009:DCP:1640541.1640848}, the Barton Libraries Dataset~\cite{barton-benchmark}
WordNet~\cite{Miller:1995:WLD:219717.219748} etc.) is provided in paper~\cite{Duan:2011:AOC:1989323.1989340}. The authors focus mainly on \emph{structuredness} (\emph{coherence}) of each benchmark dataset claiming that a primitive metric (e.g., the number of triples or the average in/outdegree) quantifies only some specific aspect of each dataset. The level of structuredness of a dataset $D$ with respect to a type $T$ is determined by how well the instance data in $D$ conforms to type $T$. The type system is extracted from the data set looking for triples whose property is \texttt{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type} and extracting type $T$ from their object. Properties of $T$ are determined as the union of all the properties that the instances of type $T$ have. The structuredness is then expressed as a weighted sum of share of set properties of each type, whereas higher weights are assigned to types with more instances.

The authors show that structuredness of the chosen benchmarks is fixed, whereas real-world RDF datasets lie in currently untested parts of the spectrum. As a consequence, they introduce a new benchmark that accepts as input any dataset along with a desired level of structuredness and size (smaller than the size of the original data), and uses the input dataset as a seed to produce a subset of the original data with the indicated size and structuredness. In addition they show that structuredness and size mutually influence each other.


\paragraph{DBPSB} DBpedia SPARQL Benchmark (DBPSB)~\cite{Morsey2011,Morsey:2012:UBR:2900929.2901031} proposed at the University of Leipzig is based on queries that were issued by humans and applications
against existing RDF data. In addition, the authors argue that benchmarks like LUBM, BSBM, or SP$^2$Bench resemble relational database benchmarks involving relational data structures with few and homogeneously structured classes, whereas RDF knowledge bases are increasingly heterogeneous (e.g., DBpedia version 3.6, for example, contains 289,016 classes of which 275 classes belong
to the DBpedia ontology) and various datatypes and object references of different types are
used in property values. Hence, they propose a generic SPARQL benchmark creation methodology based on: a flexible data generation mimicking an input data source, query-log mining, clustering, and SPARQL feature analysis.

The proposed dataset creation process starts with an input dataset. Datasets of multiple size of the original data are created by duplicating all triples and changing their namespaces.  For generating smaller datasets, an appropriate fraction of all triples is selected randomly or by sampling across classes in the dataset. The goal of the query analysis and clustering is to detect prototypical queries on the basis of their frequent usage and similarity. The methodology is applied on the DBpedia SPARQL endpoint and a set of 25 SPARQL query templates is derived, which cover most commonly used SPARQL features and are used to generate the actual benchmark queries by parametrization.

\paragraph{LODIB} The Linked Open Data Integration Benchmark (LODIB)~\cite{DBLP:conf/www/RiveroSBR12} aims to reflect the real-world heterogeneities that exist on the Web of Data in order to enable testing of Linked Data translation systems. It provides a catalogue of 15 data translation patterns (e.g., rename class, remove language tag etc.), each of which is a common data translation problem in the context of Linked Data. The benchmark provides a data generator that produces three different synthetic data sets that need to be translated
by the system under test into a single target vocabulary. They  reflect the pattern distribution in analyzed 84 data translation examples from the LOD Cloud. The data sets reflect the same e-commerce scenario used for BSBM.


\paragraph{SIB} The authors of the Social Network Intelligence BenchMark (SIB)~\cite{sib} claim that current benchmarks are limited in representing the real RDF datasets and are mostly relational-like. Hence, they propose a benchmark for challenging query processing over real graphs. It simulates an RDF backend of a social network site, in which users and their interactions form an social graph of social activities such as writing posts, posting comments, creating/managing groups, etc. The distribution of generated data on each relation conforms to the data distribution analyzed over real-world social networks. Association rules are included in order to convey the real-world data correlation into synthetic data. The  generated data is linked with the RDF datasets from DBpedia. The benchmark specification contains 3 query mixes -- interactive, update, and analysis -- expressed in SPARQL 1.1 Working Draft.

\paragraph{Geographica} The authors of benchmark Geographica~\cite{DBLP:conf/semweb/GarbisKK13} target the area of geospatial data and respective SPARQL extensions GeoSPARQL and stSPARQL. The benchmark involves (1) a real-world workload based on publicly available linked data sets covering a range of geometry types (e.g., points, lines, polygons) and (2) a synthetic workload. In the former case they further distinguish (1) a micro benchmark that tests primitive spatial functions (involving 29 queries) and (2) macro benchmark that tests the performance of RDF stores in typical application scenarios like reverse geocoding or map search and browsing (consisting of 11 queries). In the latter case of a synthetic workload the generator produces synthetic datasets of various sizes that corresponds to an ontology based on OpenStreetMap (i.e., states in a country, land ownership, roads and  points of interest) and instantiates query templates. The spatial extent of the land ownership dataset constitutes a uniform grid of $n \times n$ hexagons, whereas the size of each dataset is given relatively to $n$. The synthetic workload generator produces SPARQL queries corresponding to spatial selection and spatial joins by instantiating 2 query templates.


\paragraph{WatDiv} The Waterloo SPARQL Diversity Test Suite (WatDiv)~\cite{Aluc:2014:DST:2717213.2717229} developed at the University of Waterloo provides stress testing tools to address the observation that existing SPARQL benchmarks are not suitable for testing systems for diverse queries and varied workloads. The authors introduce two classes of query features -- structural and data-driven -- and perform an in-depth analysis on existing SPARQL benchmarks (LUBM, BSBM, SP$^2$Bench, and DBPSB) using the two classes of query features. The structural features involve triple pattern count, join vertex count, join vertex degree, and join vertex count. The data-driven features involve result cardinality and several types of selectivity. The analysis of the four benchmarks reveals that they are not sufficiently diverse to test the strengths and weaknesses of different physical design choices employed by RDF systems.

The proposed solution, WatDiv, involves (1) a data generator which generates scalable datasets according to WatDiv schema, (2) a query template generator which traverses the WatDiv schema and generates a diverse set of query templates, (3) a query generator which instantiates the templates with actual RDF terms from the dataset, and (4) a feature extractor which computes the structural and data-driven features of the generated data and workload. For the study in the paper the authors generated 12,500 test queries from 125 query templates.

\paragraph{RBench} RBench~\cite{Qiao:2015:RAR:2723372.2746479} is an application-specific benchmark which takes any RDF dataset as a template, and generates a set of synthetic datasets with similar characteristic, the required size scaling factor $s$ and the (node) degree scaling factor $d$. A generated benchmark dataset is considered similar to the given dataset if their values for the dataset evaluation metrics and query evaluation times for different techniques are similar. Three evaluation metrics -- dataset coherence (i.e., a measure how uniformly predicates are distributed among the same type/class), relationship specialty (i.e., the number of occurrences of the same predicate associated with each resource), and literal diversity -- are utilized for this purpose. A query generation process is proposed to generate 5 types of queries (node queries, edge queries, star queries, path queries, and subgraph queries) for any generated data.

The benchmark project FEASIBLE~\cite{Saleem2015} is also an application-specific benchmark; however, contrary to RBench, it is able to generate benchmarks from a set of queries (in particular from query logs) by selecting prototypical queries of a user-defined
size from the input set of queries.


\paragraph{LDBC}  The Linked Data Benchmark Council\footnote{\url{http://ldbcouncil.org/industry/organization/origins}} (LDBC)~\cite{Angles:2014:LDB:2627692.2627697} is a result of a (closed) EU project that brought together a community of academic researchers and industry, whose main objective was the development of open source, yet industrial grade, benchmarks for graph and RDF databases. The following three benchmarks were developed and are currently maintained.


The Semantic Publishing Benchmark (SPB)~\cite{spb} is inspired by the Media/Publishing industry (namely BBC\footnote{\url{http://www.bbc.com/}}). The application scenario considers a media or a publishing organization that deals with large volume of streaming content, namely news, articles or  ``media assets''. This content is enriched with metadata that describes it and links it to reference knowledge -- taxonomies and databases that include relevant concepts, entities and factual information. The SPB data generator produces scalable in size synthetic large data. Synthetic data consists of a large number of annotations of media assets that refer entities found in reference datasets. The data generator models three types of relations in produced synthetic data: (1) clustering of data, (2) correlations of entities, and (3) random tagging of entities. Two workloads are provided: (1) basic, involving an interactive query-mix, querying the relations between entities in reference data, and (2) advanced,  consisting of interactive and analytical query-mixes.

The Social Network Benchmark (SNB) models a social network akin to Facebook (see Section~\ref{sec:generators_socialnetworks}).

Graphalytics~\cite{Iosup:2016:LGB:3007263.3007270} is an industrial-grade benchmark for graph analysis platforms. It involves 6 real-world datasets and 2 synthetic datasets generated by LDBC SNB (see Section~\ref{}) and Graph500 (see Section~\ref{}). The benchmark involves 6 algorithms: breadth-first search, PageRank, weakly connected components, community detection using label propagation, local clustering coefficient, and single-source shortest paths. 

LinkGen~\cite{10.1007/978-3-319-46547-0_12}


