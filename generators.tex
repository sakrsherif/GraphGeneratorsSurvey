\section{Graph Data Generators}
\label{sec:generators}

In this section, we discuss the various graph data generators based on the classification introduced before in more detail. For each category, we first describe the key features of each of the representative examples and summarize their strengths and weaknesses. The goal is to offer a detailed information about each of the tools in the context of its competitors from the same domain.


\subsection{General Graphs}
\label{sec:generators_general}

We start by focusing on approaches that have been designed for dealing with the
generation of general graph data that is not aimed at a particular application
domain. In general, such generators focus on reproducing properties observed in
real graphs regardless of their domain such as the degree distribution, the
diameter, the presence of a large connected component, a community structure or
a significantly large clustering coefficient.

%Currently, there exists a number of tools which involve a kind of
%general graph data generator, such as gtools from projects nauty and
%Traces~\cite{gtools} or the Stanford GraphBase~\cite{GraphBase}. We, however,
%focus on primarily generating/benchmarking projects targeting the Big Data
%world.


\paragraph {Preferential Attachment} Barabasi and
Albert~\cite{Barabasi99emergenceScaling} introduced a graph generation model
that relies on two main mechanisms. The first mechanism is continuously
expanding the graphs by adding new vertices. The second mechanism is to
preferentially attach the new vertices   to the nodes/regions that are already
well connected. So, in this approach, the generation of large graphs is governed
by standard, robust self-organizing mechanisms that go beyond the
characteristics of individual applications.

\paragraph {R-MAT} R-MAT (\emph{R}ecursive \emph{Mat}rix) is a procedural
synthetic graph generator which is designed to generate power-law degree
distributions~\cite{DBLP:conf/sdm/ChakrabartiZF04}.  The generator is recursive
and employs a fairly small number of parameters.  In principle, the strategy of
this generator is to achieve simple means to produce graphs whose properties correspond to
properties of the real-world graphs. In particular, the design goal of R-MAT is to
produce graphs which mimics the degree distributions, imitate a community
structure and have a small diameter. R-MAT can generate weighted, directed and
bipartite graphs.


\paragraph{HPC Scalable Graph Analysis Benchmark} The HPC Scalable Graph
Analysis Benchmark~\cite{HPCgraph,Bader:2005:DIH:2099301.2099360} consists of a
weighted, directed graph that has a power-law distribution and four related
analysis techniques (namely graph construction, graph extraction with BFS,
                     classification of large vertex sets, and graph analysis
                     with betweenness centrality). The generator has the
following parameters: the number of nodes, the number of edges, and maximum weight of
an edge. It outputs a list of tuples containing identifiers of vertices of an
edge (with the direction from the first one to the second one) and weights
(positive integers  with a uniform random distribution) assigned to the edges of
the multigraph.  The algorithm of the generator is based on R-MAT.
%To reduce the data
%locality, the vertex ids are randomly permuted and the output
%tuples are shuffled. A related project from the same authors
%developed for the 9th DIMACS Shortest Paths Challenge is GTgraph~\cite{GTgraph}.
%t involves three types of graphs: (1) Erd\"{o}s-R\'{e}nyi random graphs, (2)
%different graphs from the DARPA HPCS SSCA\#2 graph theory benchmark
%(version 1.0), and (3) small-world graphs generated using the R-MAT graph
%generator.



\paragraph{GraphGen} For the purpose of testing the scalability of an indexing
technique called FG-index~\cite{Cheng:2007:FTV:1247480.1247574} on the size of the
database of graphs, their average size and average density, the authors have
also implemented a synthetic generator called
GraphGen\footnote{https://www.cse.ust.hk/graphgen/}. It relies on data generation code for associations and sequential
patterns provided by IBM\footnote{From 1996, no longer available at
\url{http://www.almaden.ibm.com/cs/projects/iis/hdb/Projects/data
mining/mining.shtml}}. GraphGen yields a collection of undirected, labeled and
connected graphs. It addresses the performance evaluation of frequent subgraph mining
algorithms and graph query processing algorithms. The result is represented as a
list of graphs, each consisting of a list of nodes along with a list of edges.


%\paragraph{Graph 500 Benchmark} The Graph 500 Benchmark~\cite{Graph500} includes
%a scalable data generator which produces weighted, undirected graph as a list of
%edge tuples containing the label of start vertex and end vertex together with a
%weight that represents data assigned to the edge. The space of vertex  labels is
%the set of integers beginning with 0. The input values required to describe the
%graph are (1) scale, i.e., the logarithm base two of the number of vertices, and
%(2) edge factor, i.e., the ratio of the graph's edge count to its vertex count
%(i.e., half the average degree of a vertex in the graph). The graph generator is
%a Kronecker generator similar to R-MAT. The data must not exhibit any locality,
%so in the final step the vertex labels and order of edges are randomly shuffled.
%The covered operations currently involve BFS; however, the authors intend to
%involve also two more types -- optimization (single source shortest path) and
%edge-oriented (maximal independent set) -- and five graph-related business areas:
%cybersecurity, medical informatics, data enrichment, social networks, and
%symbolic networks.

\paragraph{BTER} BTER (Block Two-Level
Erd\"{o}s-R\'{e}ny)~\cite{kolda2014scalable} is a graph generator based on the
creation of multiple Erd\"{o}s-R\'{e}ny graphs with different connection
probabilities  of which they are connected randomly between them. As the main feature, BTER is able
to reproduce input degree distributions and average clustering
coefficient per degree values. The generator starts by grouping the vertices
by degree $d$, and forming groups of size $d+1$ of nodes with degree $d$. Then, these
groups are assigned an internal edge probability in order to match the observed
average clustering coefficient of the nodes of such degree. Based on this
probability, for each node, the excess degree (i.e, the degree that in
expectation will not be realized internally in the group) is computed and used to connect
nodes from different groups at random. The authors describe a highly scalable
MapReduce-based implementation that is able to generate large graphs (consisting of
billions of nodes) in a reasonable amounts of time.

\paragraph{Darwini} Darwini~\cite{edunov2016darwini} is an extension of BTER
designed to run on Vertex Centric computing frameworks like
Pregel~\cite{malewicz2010pregel} or Apache Giraph~\cite{ching2015one}, with the
additional feature that it is more accurate when reproducing the clustering
coefficient of the input graph. Instead of just focusing on the average
clustering coefficient for each degree, Darwini is able to model the clustering
coefficient distribution per degree. It achieves this by gathering the nodes
of the graph into buckets based on the expected number of closed triangles that they need
to close in order to attain the expected clustering coefficient. The latter is
sampled from the input distributions. Then, the vertices in each bucket are
connected randomly with a probability that would produce the expected
desired number of triangles for each bucket. Then, as in BTER, the excess degree
is used to connect the different buckets. The authors report that Darwini is
able to generate graphs with billions and even trillions of edges.

%\paragraph{gtools} Projects nauty and Traces~\cite{gtools} are programs for
%computing automorphism groups of graphs and digraphs~\cite{McKay201494}; they
%can also produce a canonical label. In the respective package there is also
%suite of programs called gtools which involve generators for non-isomorphic
%graphs, bipartite graphs, digraphs, and multigraphs.

\paragraph{RTG} \emph{The Random Typing Generator} (RTG)~\cite{DBLP:journals/datamine/AkogluF09} aims at generating realistic graphs. In particular, it outputs (un)weighted, (un)directed, as well as uni/bipartite graphs, whereas the realism of the output is ensured by 11 laws (e.g., densification power law, weight power law, small and shrinking diameter, community structure, etc.) known to be typically exhibited by real-world graphs. On input it requires 4 parameters ($k$, $q$, $W$, and $\beta$) that correspond to the core Miller's observation~\cite{miller1957} that a random process (namely, having a keyboard with $k$ characters and a space, the process of random typing of $W$ words, where the probability of hitting a space is $q$ and the probability of hitting any other characters is $(1-q)/k$) leads to Zipf-like power laws (of the resulting words) plus in addition (using imbalance factor $\beta$) ensure homophily and community structure. RTG is based on the idea creating an edge between pairs of consecutive words.

Paper~\cite{DBLP:conf/wbdb/AmmarO13} further extends this idea mainly in the direction of simplification of specifying of the parameters. Instead of Miller's parameters that are not much associated with graphs, the authors prove and exploit their relationship with size and density of the target graph.

\paragraph{Strengths and Weaknesses of General Graph Generators}
In general, existing general graph generators produce graphs with the following
properties: skewed degree distribution (e.g., powerlay), small diameter, a large largest connected component, large
clustering coefficient, and some degree of
community structure. Degree distribution can be typically configured while other
properties are just a result of the generation process and cannot be controlled
by any means.

This is not the case of BTER and Darwini, which besides the degree distribution,
they also allow tuning of the clustering coefficient. However, some use cases
demand for more control on the characteristics of the generated graphs.
This is the case, for example, of benchmarking, where the underlying graph
structure has direct implications to the performance of the graph algorithms
to run. For this reason, the design of graph generators with the capability of fine
tuning the characteristics of the generated graphs still remains as an open
challenge. The questions that remain are what characteristics to tune
and which are the algorithms that depend on such characteristics.


\subsection{Semantic Web}
\label{sec:generators_LinkedData}
With the dawn of the concept of Linked Data it is a natural development that there would emerge respective
benchmarks involving both synthetic data and  data sets   with real-world characteristics.
The used data sets correspond to RDF representation of relational-like data~\cite{Guo2005158,Bizer09theberlin}, social network-like data~\cite{Schmidt2010}, or specific and significantly more complex data structures such as biological data~\cite{Wu2014}. In this section, we provide an overview of benchmarking systems involving a kind of graph-based RDF data generator or data modifier. %Other types of systems or particular results can be found, e.g., at~\cite{RdfStoreBenchmarking}.


\iffalse
Considering the Big Data world, the Linked Data in general definitely belong to this group since we assume that the Linked (Open) Data Sets form a common Linked Open Data cloud\footnote{\url{http://lod-cloud.net/}}. On the other hand, the particular data sets can be relatively small.
\fi

\paragraph{LUBM} The use-case driven Lehigh University Benchmark (LUBM)\footnote{\url{http://swat.cse.lehigh.edu/projects/lubm/}} considers the university domain. The ontology defines 43 classes and 32 properties~\cite{Guo2005158}. In addition, 14 test queries are provided in the LUBM benchmark. In particular, the benchmark focuses on \emph{extensional} queries, i.e., queries which target the particular data instances of the ontology, as an opposite to \emph{intentional} queries, i.e., queries which target properties and classes of the ontology. The Univ-Bench Artificial  (UBA) data generator features repeatable and random  data generation (exploiting classical linear congruential generator, LCG, of numbers). In particular, the data which is produced by the generator are assigned zero-based indexes (i.e., \emph{University0}, \emph{University1} etc.), thus they are reproducible at any time with the same indexes.  The generator naturally allows to specify a seed for random number generation, along with the starting index and the desired number of universities.

An extension of LUBM, the Lehigh BibTeX Benchmark (LBBM)~\cite{Wang2005}, enables generating synthetic data for different ontologies. The data generation process is managed through two main phases: (1) the property-discovery phase, and (2) the data generation phase. LBBM provides a probabilistic model that can emulate the discovered properties of the data of a particular domain and generate synthetic data exhibiting similar properties. Synthetic data are generated using a Monte Carlo algorithm. The approach is demonstrated on the Lehigh University BibTeX ontology which consists of 28 classes along with 80 properties. The LUBM benchmark includes 12 test queries that were designed for the benchmark data. Another extension of LUBM, the University Ontology Benchmark (UOBM)\footnote{\url{https://www.cs.ox.ac.uk/isg/tools/UOBMGenerator/}}, focuses on two aspects: (1) usage of all constructs of OWL Lite and OWL DL~\cite{owl} and (2) lack of necessary links between the generated data which thus form isolated graphs~\cite{Ma:2006:TCO:2094613.2094629}. In the former case the original ontology is replaced by the two types of extended versions from which the user can choose. In the latter case cross-university and cross-department links are added to create a more complex graph.

\paragraph{IIMB} Ferrara et al.~\cite{Ferrara08OM} proposed the ISLab Instance Matching Benchmark (IIMB)\footnote{\url{http://www.ics.forth.gr/isl/BenchmarksTutorial/}} for the problem of instance matching. For any two objects $o_1$ and $o_2$ adhering to different ontologies or to the same ontology, instance matching is specified in the form of a function $Om(o_1, o_2) \rightarrow \{0, 1\}$,  where $o_1$ and $o_2$ are linked to the same real-world object (in which case the function maps to $1$) or $o_1$ and $o_2$ are representing different objects (in which case the function maps to $0$). It targets the domain of movie data which contains 15 named classes, along with 5 objects and 13 datatypes. The data are extracted from IMDb\footnote{\url{http://www.imdb.com/}}. The data generator corresponds to a data modifier which simulates differences between the data. In particular it involves data value differences (such as typographical errors or usage of different standard formats, e.g., for names), structural heterogeneity (represented by different levels of depth for properties, diverse aggregation criteria for properties, or missing values specification) and logical heterogeneity (such as, e.g., instantiation on disjoint classes or various subclasses of the same superclass).


\paragraph{BSBM} The Berlin SPARQL Benchmark (BSBM)\footnote{\url{http://wifo5-03.informatik.uni-mannheim.de/bizer/berlinsparqlbenchmark/}}, is centered around an e-commerce application domain with object types such as \emph{Customer}, \emph{Vendor}, \emph{Product} and \emph{Offer} in addition to the relationship among them~\cite{Bizer09theberlin}.
The benchmark provides a workload that has 12 queries with 2 types of query workloads (i.e., 2 sequences of the 12 queries) emulating the navigation pattern and search of a consumer seeking a product. The data generator is capable of producing arbitrarily scalable datasets by controlling the number of products ($n$) as a scale factor.  The scale factor also impacts other data characteristics, such as, e.g., the depth of type hierarchy of products, branching factor, the number of product features,  etc. BSBM can output two representations, i.e. an RDF representation along with a relational representation. Thus, BSBM also defines an SQL~\cite{sql} representation of the queries. This allows comparison of SPARQL~\cite{sparql} results  to be compared against the performance of traditional RDBMSs.


\paragraph{SP$^2$Bench} The SP$^2$Bench\footnote{\url{http://dbis.informatik.uni-freiburg.de/forschung/projekte/SP2B/}}
is a language-specific benchmark~\cite{Schmidt2010} which is based on the DBLP dataset. %, so the types involve Person, Inproceedings, Article etc.
The generated datasets follow the key characteristics of the original DBLP dataset. In particular, the data mimics the correlations between entities. All random functions of the generator use a fixed seed that ensures that the data generation process is deterministic. SP$^2$Bench is accompanied by 12 queries covering the various types of operators such as RDF access paths in addition to typical RDF constructs.

%\paragraph{JustBench} ~\cite{Bail:2010:JFO:1940281.1940285} ...




\paragraph{DBPSB} DBpedia SPARQL Benchmark (DBPSB)\footnote{\url{http://aksw.org/Projects/DBPSB.html}} proposed at the University of Leipzig has been designed using workloads that have been generated by applications and humans~\cite{Morsey2011,Morsey:2012:UBR:2900929.2901031}. In addition, the authors argue that benchmarks like LUBM, BSBM, or SP$^2$Bench resemble relational database benchmarks involving relational-like data which is structured using a small amount of homogeneous classes, whereas, in reality, RDF datasets are tending to be more heterogeneous. For example, DBpedia 3.6 consists of 289,016 classes, whereas 275 of them are defined based on the DBpedia ontology. In addition, in property values different data types as well as references to objects of the various types are
used. Hence, they presented a universal SPARQL benchmark generation approach which uses a flexible data production mechanism that mimics the input data source. This dataset generation process begins using an input dataset; then multiple datasets with different sizes  are then generated by duplicating all the RDF triples with changing their namespaces.  For generating smaller datasets, an adequate selection of all triples is selected randomly or using a sampling mechanism over the various classes in the dataset. \iffalse The goal of the query analysis and clustering is to detect prototypical queries on the basis of their frequent usage and similarity.\fi The methodology is applied on the DBpedia SPARQL endpoint and a set of 25 templates of SPARQL queries is derived to cover frequent SPARQL features.

\paragraph{LODIB} The Linked Open Data Integration Benchmark (LODIB)\footnote{\url{http://lodib.wbsg.de/}} has been designed with the aim of reflecting the real-world heterogeneities that exist on the Web of Data in order to enable testing of Linked Data translation systems~\cite{DBLP:conf/www/RiveroSBR12}. It provides a catalogue of 15 data translation patterns (e.g., rename class, remove language tag etc.), each of which is a common data translation problem in the context of Linked Data. The benchmark provides a data generator that produces three different synthetic data sets that need to be translated
by the system under test into a single target vocabulary. They  reflect the pattern distribution in analyzed 84 data translation examples from the LOD Cloud. The data sets reflect the same e-commerce scenario used for BSBM.




\paragraph{Geographica} The Geographica benchmark\footnote{\url{http://geographica.di.uoa.gr/}} has been designed to target the area of geospatial data~\cite{DBLP:conf/semweb/GarbisKK13} and respective SPARQL extensions GeoSPARQL~\cite{battle2012enabling} and stSPARQL~\cite{koubarakis2010modeling}. The benchmark involves a real-world workload that uses openly available datasets that cover various geometry elements (such as, e.g., lines, points, polygons, etc.) and  a synthetic workload. In the former case there is a (1) a micro benchmark that evaluates primitive spatial functions (involving 29 queries) and (2) macro benchmark that tests the performance of RDF engines in various application scenarios such as  map exploring and search (consisting of 11 queries). In the latter case of a synthetic workload the generator produces synthetic datasets of different sizes that corresponds to an ontology based on OpenStreetMap  and instantiates query templates. \iffalse The spatial extent of the land ownership dataset constitutes a uniform grid of $n \times n$ hexagons, whereas the size of each dataset is given relatively to $n$.\fi The generated SPARQL query workload is corresponding to spatial joins and selection using 2 query templates.


\paragraph{WatDiv} The Waterloo SPARQL Diversity Test Suite (WatDiv)\footnote{\url{http://dsg.uwaterloo.ca/watdiv/}} has
been designed at the University of Waterloo. It implements stress testing tools that focus on addressing the observation
that the state-of-the-art SPARQL benchmarks do not fully cover the variety of queries and
workloads~\cite{Aluc:2014:DST:2717213.2717229}. The benchmark focuses on two types of query aspects --
structural and data-driven -- and performs a detailed analysis on existing SPARQL benchmarks
(LUBM, BSBM, DBPSB, and SP$^2$Bench) using these two properties of queries. The structural features involve triple
pattern count, join vertex count, join vertex degree, and join vertex count. The data-driven features involve
result cardinality and several types of selectivity. The analysis of the four benchmarks reveals that their diversity is
insufficient for evaluation of the weaknesses/strengths of the distinct design alternatives implemented by the different
RDF systems.

In particular, WatDiv, provides (1) a data generator which generates scalable datasets according to the WatDiv
schema, (2) a query template generator which produces a  set of query templates according to the WatDiv schema, and
(3) a query generator that uses the generated templates and instantiates them  with real RDF values from the dataset, and (4) a feature extractor which extracts the structural features of the generated data and workload. %For the study in the paper the authors generated 12,500 test queries from 125 query templates.

\paragraph{RBench} RBench~\cite{Qiao:2015:RAR:2723372.2746479} is an application-specific benchmark which receives any RDF dataset as an input and produces a set of datasets, that have similar characteristics of the input dataset, using size scaling factor $s$ and (node) degree scaling factor $d$. These factors ensure that the original RDF graph $G$ and the synthetic graph $G'$ are similar and the average node degree and the number of edges of $G'$ are changed by $s$ and $d$ respectively. \iffalse A generated benchmark dataset is considered similar to the given dataset if their values for the dataset evaluation metrics and query evaluation times for different techniques are similar. Three evaluation metrics are utilized for this purpose: dataset coherence (i.e., a measure how uniformly predicates are distributed among the same type/class), relationship specialty (i.e., the number of occurrences of the same predicate associated with each resource), and literal diversity.\fi A query generation process has been implemented to produce 5 different types of queries (edge-based queries, node-based queries, path queries, star queries, subgraph queries) for any generated data. The benchmark project FEASIBLE~\cite{Saleem2015} is also an application-specific benchmark; however, contrary to RBench, it is designed to produce benchmarks from the set of sample input queries of a user-defined
size.

In practice, one way for handling big RDF graphs is to process them using the
\emph{streaming} mode where the data stream could consist of the edges of the
graph. In this mode, the RDF processing algorithms can process the input
stream in the order it arrives while using only a limited amount of
memory~\cite{mcgregor2014graph}. The streaming mode has mainly  attracted the attention of the
RDF and Semantic Web community.

\paragraph{S2Gen}   Phuoc et al.~\cite{le2012linked} presented
an evaluation framework for linked stream data processing engines. The framework
uses a datasets generated with the Stream Social network data Generator
(S2Gen), which
simulates streams of user interactions (or events) in social networks
(e.g., posts) in addition to the  user metadata such as users' profile
information, social network relationships, posts, photos and GPS information.
The data generator of this framework provides the users the flexibility to
control the characteristics of the generated stream by tanning a range of
parameters, which includes the frequency at which interactions are generated,
limits such as the maximum number of messages per user
and week, and the correlation probabilities between the different objects (e.g.,
users) in the social network.

\paragraph{RSPLab} Tommasini et al.~\cite{tommasini2017rsplab} introduced
another framework for benchmarking RDF Stream Processing systems, RSPLab. The
Streamer component of this framework is designed to publish RDF streams from the
various existing RDF benchmarks (e.g., BSBM, LUBM).
In particular, the Streamer  component uses TripleWave\footnote{\url{http://streamreasoning.github.io/TripleWave/}}, an
open-source framework which enables to share RDF streams on the
Web~\cite{mauri2016triplewave}.   TripleWave acts as a means for plugging-in
and combining streams from multiple Web data sources using either pull or push mode.



\paragraph{LDBC}  The Linked Data Benchmark Council\footnote{\url{http://ldbcouncil.org/industry/organization/origins}} (LDBC)~\cite{Angles:2014:LDB:2627692.2627697} %is a result of a (closed) EU project that brought together a community of academic researchers and industry that
had the goal of developing an open source, yet industrial grade benchmarks for RDF and graph databases.
\iffalse The following three benchmarks were developed and are currently maintained.\fi In the Semantic Web
domain, it released the Semantic Publishing Benchmark (SPB)~\cite{spb} that has been inspired by the
Media/Publishing industry (namely BBC\footnote{\url{http://www.bbc.com/}}). The application scenario
of this benchmark simulates a media or a publishing organization that handles large amount
of streaming content (e.g., news, articles). \iffalse This content is enriched with metadata that describes
it and links it to reference knowledge -- taxonomies and databases that include relevant concepts,
entities and factual information. The SPB data generator produces scalable in size synthetic large data.
Synthetic data consists of a large number of annotations of media assets that refer entities found in
reference datasets.\fi The data generator mimics three types of relations in the generated synthetic data:
correlations of entities, data clustering, and random tagging of entities. Two workloads are provided: (1) basic, involving an interactive query-mix querying the relations between entities in reference data, and (2) advanced,  focusing on interactive and analytical query-mixes. The LDBC has designed two other benchmarks: the Social Network Benchmark (SNB)~\cite{Erling:2015:LSN:2723372.2742786} for the social network domain  (see Section~\ref{sec:generators_socialnetworks}) and Graphalytics~\cite{Iosup:2016:LGB:3007263.3007270}   for the analytics domain.% (see Section~\ref{sec:generators_analytics}).



\paragraph{LinkGen} LinkGen is a synthetic linked data generator that has been designed to generate RDF datasets for a given vocabulary~\cite{10.1007/978-3-319-46547-0_12}. The generator is designed to receive a vocabulary as an input  and supports two statistical distributions for generating entities:  Zipf's power-law distribution and Gaussian distribution. LinkGen can augment the generated data with inconsistent and noisy  data such as updating a given datatype property with two conflicting values or  adding triples with syntactic errors. \iffalse, adding wrong statements by assigning them with invalid domain and creating instances with no type information.\fi The generator also provides a feature to inter-link the generated objects with real-world ones from user-provided real-world datasets. The datasets can be generated in any of of two modes: on-disk and streaming.


\paragraph{Strengths and Weaknesses of Semantic Web Graph Generators}  Graphs are intuitive and standard representation for the RDF model that form the basis for the Semantic Web community which has been very active on building several benchmarks, associated with graph generators that had various design principles.

A comparison of 4 RDF benchmarks (namely TPC-H~\cite{TPC-H} data expressed in RDF, LUBM, BSBM, and SP$^2$Bench) and 6 real-wold data sets (such as, e.g.,  DBpedia, the Barton Libraries Dataset~\cite{barton-benchmark} or
WordNet~\cite{Miller:1995:WLD:219717.219748}) has been reported by~\cite{Duan:2011:AOC:1989323.1989340}. The authors focus mainly on the  \emph{structuredness} (\emph{coherence}) of each benchmark dataset claiming that a primitive metric (e.g., the number of triples or the average in/outdegree) quantifies only some target characteristics of each dataset. With respect to a type $T$ the degree of structuredness of a dataset $D$  is based on  the regularity of instance data in $D$ in conforming to type $T$. The type system is extracted from the data set by finding the RDF triples that have property  \texttt{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type} and extract type $T$ from their object. Properties of $T$ are determined as the union of all properties of type $T$. The structuredness is then expressed as a weighted sum of share of set properties of each type, whereas higher weights are assigned to types with more instances. The authors show that the structuredness of the chosen benchmarks is fixed, whereas real-world RDF datasets are belonging to the non-tested area of the spectrum. As a consequence, they introduce a new benchmark that receives as input any dataset associated with a required level of structuredness and size (smaller than the size of the original data), and exploits the input documents as a seed to produce a subset of the original data with the target structuredness and size. In addition, they show that structuredness and size mutually influence each other.

With the recent increasing momentum of streaming data, the Semantic Web community started to consider the issues and challenges of RDF streaming data. However, there is still a lot of open challenges that needs to tackled in this direction such as covering different real-world application scenarios.

\subsection{Graph Databases}
\label{sec:generators_GraphDatabases}

Currently there exists a number of papers which compare the efficiency of graph databases with regards to distinct use cases, such as  the community detection problem~\cite{Beis2015}, social tagging systems~\cite{Giatsoglou2011}, graph traversal~\cite{Ciglan:2012:BTO:2374486.2375242}, graph pattern matching~\cite{Pobiedina2014}, data provenance~\cite{Vicknair:2010:CGD:1900008.1900067}, or even several distinct use cases~\cite{Grossniklaus2013Towar-24253}. However, the number of graph data generators and benchmarks that have been designed specifically for graph data management systems (Graph DBMS)  is relatively small. Either a general graph generator is used for benchmarking graph databases, such as, e.g., the HPC Scalable Graph Analysis Benchmark~\cite{Dominguez-Sal:2010:SGD:1927585.1927590} or the graph DBMS benchmarking tools are designed while having in mind a more general scope. Hence it is questionable whether a benchmark  that is targeted specifically for graph databases is necessary. \cite{Dominguez-Sal:2010:DDG:1946050.1946053} discussed this question and related topics. On the basis of a review of applications of graph databases (namely, social network analysis,  genetic interactions and recommendation systems), the authors analyzed and discussed the features of the graphs for these types of applications and how such features can affect the benchmarking process, various types of operations used in these applications and the characteristics of the evaluation setup of the benchmark. In this section, we focus on graph data generators and benchmarks that have been primarily targeting graph DBMSs.


\paragraph{XGDBench} XGDBench~\cite{Dayarathna:2014:GDB:2676904.2676939} is an extensible  benchmarking platform for graph databases used in cloud-based systems. Its intent is to automate
benchmarking of graph databases in the cloud by focusing on the domain social networking services. It extends the Yahoo! Cloud Multiplicative Attribute (MAG) Graph Serving Benchmark (YCSB)~\cite{Cooper:2010:BCS:1807128.1807152} and provides a set of standard workloads representing various performance issues. In particular, the workload of XGDBench involves basic operations such as read / insert / update / delete an attribute, loading of the list of neighbours and BFS traversal. Using the generators, 7 workloads are created, such as update heavy, read mostly, short range scan, traverse heavy etc.
The data model of XGDBench is a simplified version of the Multiplicative Attribute Graph (MAG)~\cite{Kim2010} model, a synthetic graph model which models the interactions between node attributes and  graph structure.
The generated graphs are thus in MAG format, with power-law degree distribution closely simulating real-world social networks.
The simplified MAG algorithm accepts the required number of nodes and for each node the number of attributes, a threshold for random  initialization of attributes, a threshold for edge affinity which determines the existence of an edge between two nodes, and an affinity matrix. \iffalse It has been proven that MAG generates graphs with both analytically tractable and statistically interesting properties.\fi
Large graphs can be generated on multi-core systems by XGDBench multi-threaded version.


\paragraph{gMark}  gMark~\cite{gMark} is a schema-driven and domain-agnostic generator of both graph instances and graph query workloads. It can generate instances under the form of N-triples and queries in various concrete query languages, including OpenCypher\footnote{\url{https://neo4j.com/developer/cypher-query-language/}}, recursive SQL, SPARQL and LogicQL. In gMark, it is possible to specify a \emph{graph configuration} involving the admitted edge predicates and node labels occurring in the graph instance along with additional parameters such as degree distribution, occurrence constraints, etc. The \emph{Query workload configuration} describes parameters of the query workload to be generated, by including the number of queries, arity, shape and selectivity of the queries.
The problem of deciding whether there exists a graph that satisfies a defined graph specification $G$ is NP-complete. The same applies to the problem of deciding
whether there exists a query workload compliant with a given query workload configuration $Q$. In view of this, gMark adopts a best effort approach in which the
parameters specified in the configuration files are attained in a relaxed fashion in order to achieve linear running time whenever possible.

%The authors prove that deciding whether there exists a graph satisfying a given graph configuration $G$ is NP-complete. And, similarly, deciding
%whether there exists a query workload satisfying a given query workload configuration $Q$ is also NP-complete. Hence, gMark generating is based on a heuristic strategy: it tries to achieve the exact values of the given parameters, however, in order to obtain linear running time it may decide to relax some. gMark generates graphs under the form of N-triples and query workloads in four concrete syntaxes, including Cypher\footnote{\url{https://neo4j.com/developer/cypher-query-language/}}, SPARQL, SQL and LogicQL.

\paragraph{GraphGen}  GraphAware GraphGen\footnote{\url{http://graphgen.graphaware.com/}} is a graph generation engine based on Neo4j's\footnote{\url{https://neo4j.com/}} query language OpenCypher~\cite{GraphGen}.  It creates nodes and relationships based on a schema definition expressed in Cypher, and it can also generate property values on both
nodes and edges. As such, GraphGen is a precursor of property graphs generators. The resulting graph can be exported to several formats (namely GraphJson\footnote{\url{https://github.com/GraphAlchemist/GraphJSON/wiki/GraphJSON}} and CypherQueries) or loaded directly to a DBMS. However, it is very likely that it is not maintained anymore due to the lack of available recent commits.

\paragraph{Strengths and Weaknesses of Graph Database Generators}
The graph DBMS generators discussed in this section have in common the fact that they can generate semantically rich labeled graphs with properties (ranging from properties values in GraphGen to MAG structures in XGDBench). They are also capable of generating graph instances and query workloads in
concrete syntaxes (among which OpenCypher in GraphGen and gMark) and one of them (XGDBench) can also handle update operations on both graph structure and content. However, more comprehensive graph DBMS generators that also produce data manipulation operations (such as updates for graph databases) are urgently needed. Additionally, none of these generators is enabled to work on corresponding query languages for property graphs, such as the newly emerging standard GQL~\cite{gql-2018} and G-Core~\cite{AnglesABBFGLPPS18}. Hence, a full-fledged graph DBMS generator for property graphs and property graph query workloads~\cite{BFVY18} is still missing and there exists an interesting opportunity to build such a generator in the near future.

Another apparent inconvenience is represented by the fact that explicit correlations among graph elements cannot be encoded for instance in gMark or GraphGen, whereas they could be fruitful in order to reproduce the behavior of real-world graphs in which attribute values are correlated one with another. On the other hand, social network and Linked Data generators that support correlations (as highlighted in Section \ref{sec:generators_socialnetworks} and Section \ref{sec:generators_LinkedData}) typically exhibit a fixed schema and are not not necessarily multi-domain as are some of the graph DBMS generators discussed in this section (namely GraphGen and gMark).



\subsection{Social Networks}
\label{sec:generators_socialnetworks}

On-line social networks, like Facebook, Twitter, or LinkedIn, have become a
phenomenon used by billions of people every day and thus providing extremely
useful information for various domains. However, an analysis of such type of
graphs has to cope with two problems: (1) availability of the data and (2)
privacy of the data. Hence, data generators which provide realistic synthetic
social network graphs are in a great demand.

In general, any analysis of social networks identifies their various specific
features~\cite{Chakrabarti:2006:GML:1132952.1132954}. For example, a social
networks graph often has a high  degree of
transitivity of the graph (so-called \emph{clustering coefficient}). Or, its diameter, i.e., the longest shortest path
amongst some fraction (e.g. 90\%) of all connected nodes, is usually low due to
weak ties joining faraway cliques.

Another key aspect of social networks is the community effect. A detailed
study of structure of communities in 70 real-world networks is provided, e.g.,
in~\cite{Leskovec:2008:SPC:1367497.1367591}.
~\cite{Prat-Perez:2014:CSS:2621934.2621942} analyzed the structure of
communities (clustering coefficient, triangle participation ratio,
diameter, bridges, conductance, and size) in both real-world graphs and outputs of
existing graph generators such as LFR~\cite{PhysRevE.78.046110} and the
LDBC SNB~\cite{Erling:2015:LSN:2723372.2742786}. They found out that discovered
communities  in different graphs have common distributions and that communities
of a single graph have different characteristics and are challenging to be represented using a single
model.

The existing social network generators try to reproduce different aspects of the
generated network. They can be categorized into statistical and agent-based.
\emph{Statistical
approaches}~\cite{PhysRevE.78.046110,Yao2011,Armstrong:2013:LDB:2463676.2465296,Pham2013,Sukthankar-SocialInfo2014,Erling:2015:LSN:2723372.2742786,Nettleton2016}
focused on reproducing aspects of the network. In \emph{agent-based
approaches}~\cite{Barrett:2009:GAL:1995456.1995598,Bernstein:2013:SAS:2499604.2499609}
the networks are constructed by directly simulating the agents' social choices.

%\paragraph{LFR} Lancichinetti, Fortnato and Radicchi (hence
%LFR)~\cite{PhysRevE.78.046110} develop a class of benchmark graphs whose nodes
%participate in internal community structures. The benchmark models directed and
%weighted real-world networks (e.g., social networks) containing overlapping
%communities of different sizes. The algorithm assumes that both the degree and
%the community size distributions are power laws. Each node shares a fraction $(1
%- \mu)$ of its links with the other nodes of its community and a fraction $\mu$
%with the other nodes of the network, where $\mu$ is called \emph{mixing
%parameter}. The sizes of the communities are taken from a power law distribution
%such that the sum of all sizes equals the number of nodes of the graph. The
%generation process starts with an empty graph and incrementally fills in the
%adjacency matrix by obeying the described constraints.

\paragraph{Realistic Social Network}
~\cite{Barrett:2009:GAL:1995456.1995598} focused on the construction of
realistic social networks. For this purpose the authors combine both
private and public data sets with large-scale agent-based techniques. The process works as
follows: In the first step it generates  synthetic data by combining public and
commercial databases. In the second step, it determines a set of activity
templates. A 24-hour activity sequence including geolocations is assigned to
each synthetic individual. To demonstrate the approach, the authors create a
synthetic US population consisting of people and households together with
respective geolocations. For this purpose the authors combine simulation and
data fusion techniques utilizing various real-world data sources such as U.S.
Census data, responses to a time-use survey or an activity survey.
%Demographic information for each person and location, a minute-by-minute
%schedule of each person's activities, and the locations where these activities
%take place is generated by a combination of simulation and data fusion
%techniques.
The result is captured by a dynamic network of social contacts. Similar methods for
agent-based strategies have been reported
in~\cite{Bernstein:2013:SAS:2499604.2499609}.

\paragraph{Linkage vs. Activity Graphs}
\cite{Yao2011} distinguished between two types of social network graphs -- the
\emph{linkage graph}, where nodes correspond to people and edges correspond to their
friendships, and the \emph{activity graph}, where nodes also represent people
but edges correspond to their interactions. On the basis of the analysis of
Flickr\footnote{\url{https://www.flickr.com/}} social links and
Epinions\footnote{\url{http://www.epinions.com/}} network of user interactions,
the authors discover that they both exhibit high clustering coefficient
(community structure), power-law degree distribution and small diameter.
Considering the dynamic properties they both have relatively stable clustering
coefficient over time and follow the densification law. On the other hand,
diameter shrinking is not observed in Epinions activity graph and there is a
difference in degree correlation (i.e., frequency of mutual connections of
similar nodes) -- activity graphs have neutral, whereas linkage graphs have positive degree correlation. With regards to the findings, the
proposed generator focusses on linkage graphs with positive degree correlation.
For this purpose it extends the forest fire spreading process
algorithm~\cite{Leskovec:2005:GOT:1081870.1081893} with link symmetry. It has
two parameters: the \emph{symmetry
probability} $P_s$ and the \emph{burning probability} $P_b$. $P_b$ ensures a forward burning process based on BFS in which
fire burns strongly with $P_b$ approaching 1.  $P_s$ ensures backward linking
from old nodes to new nodes and ``adds fuel to the fire as it brings more
links''. %It gives chances for big nodes to connect back to big nodes.


\paragraph{LinkBench} The LinkBench
benchmark~\cite{Armstrong:2013:LDB:2463676.2465296} has been designed for the
purpose of analysis of efficiency of a database storing Facebook's production
data. The benchmark considers true Big Data and related problems with sharding,
replication etc. The social graph at Facebook comprises objects (nodes with IDs,
version, timestamp and data) and associations (directed edges, pairs of node
IDs, with visibility, timestamp and data). The size of the target graph is the
number of nodes. Graph edges and nodes are generated concurrently during bulk
loading. The space of node IDs is divided into chunks which enable parallel
processing. The edges of the graph are generated in accordance with the results
of analysing  real-world Facebook data (such as outdegree distribution). A
workload corresponding to 10 graph operations (such as insert object, count the
number of associations etc.) and their respective characteristics over the
real-world data is generated for the synthetic data.

\paragraph{S3G2} The Scalable Structure-correlated Social Graph Generator
(S3G2)~\cite{Pham2013} is a general framework which produces a directed labeled
graph whose vertices represent objects having property values. The respective
classes determine the structure of the properties. S3G2 does not aim at
generating near real-world data, but at generating synthetic graphs with a
correlated structure. Hence, the existing data values influence the probability
of choosing a particular property value from a pre-defined dictionary or connecting two nodes. For example, the degree distribution can be
correlated with the properties of a node and thus, e.g., people who have many
friend relationships typically post more comments and pictures. The data
generation process starts with generating a number of nodes with property values
generated according to specified property value correlations and then adding
respective edges according to specified correlation dimensions. It has multiple
phases, each focusing on one correlation dimension. Data is generated in a Map
phase corresponding to a pass along one correlation dimension. Then the data are
sorted along the correlation dimension in the following Reduce phase. A
heuristic observation that ``the probability that two nodes are connected is
typically skewed with respect to some similarity between the nodes'' enables to
focus only on sliding window of most probable candidates. The core idea of the
framework is demonstrated using an example of a social network (consisting of
persons and social activities).  The dictionaries for property values are
inspired by DBpedia and provided with 20 property value correlations. The edges
are generated according to 3 correlation dimensions.


\paragraph{SIB} The developers of the Social Network Intelligence BenchMark (SIB)\footnote{\url{https://www.w3.org/wiki/Social_Network_Intelligence_BenchMark}} based the design of their benchmark on the claim that the state-of-the-art benchmarks are limited in reflecting the characteristics of the real RDF databases and are mostly focusing on the relational style aspects. Hence, they proposed a benchmark for  query evaluation using real graphs~\cite{sib}. The proposed benchmark mimics using an RDF store for a social network. \iffalse site where users and their interactions form a social graph of social activities such as creating/managing groups, writing posts and posting comments.\fi The distribution of the generated data for each type follows the  distribution of the associated type inferred from real-world social networks. Additionally, association rules are exploited for representing the real-world data correlation in the generated synthetic data. The  generated data is linked with the RDF datasets from DBpedia. The benchmark specification contains 3 query mixes -- interactive, update, and analysis -- expressed in SPARQL 1.1 Working Draft.

\paragraph{Cloning of Social Networks} ~\cite{Sukthankar-SocialInfo2014}
introduces two synthetic generators to reproduce two characteristics typically
observed in social networks: node features and multiple link types. Both
generators extend  the generator proposed by~\cite{wang2011leveraging}.
which starts with a small number of
nodes and new nodes are added until the network reaches the required number. It
has two basic parameters: homophily and link density. A high \emph{homophily}
value reflects that links have higher chances to be established among the nodes belonging to the same community,
whereas the community membership is represented by the same labels.

The first proposed generator is Attribute Synthetic Generator (ASG), used for
reproducing the node feature distribution of standard networks and rewiring the
network to preferentially connect nodes that exhibit a high feature similarity.
The network is initialized with a group of three nodes. New nodes and links
are added to the network based on link density, homophily, and feature
similarity. As new nodes are created, their labels are assigned based on the
prior label distribution. After the network has reached the same number of nodes
as the original social media dataset, each node initially receives a random
attribute assignment. Then a stochastic optimization process is used to move the
initial assignments closer to the target distribution extracted from social
media dataset using the Particle Swarm Optimization algorithm. The tuned
attributes are then used to add additional links to the network based on the
feature similarity parameter -- a source node is selected randomly and connected
to the most similar node. The second proposed generator, so-called Multi-Link Generator
(MLG), further  uses link co-occurrence statistics from the original dataset to
create a multiplex network. MLG uses the same network growth process as ASG.
Based on the link density parameter, either a new node is generated with a label
based on the label distribution of the target dataset or a new link is created
between two existing nodes.


\paragraph{LDBC SNB} Despite having a common Facebook-like dataset, thanks to
three distinct workloads the Social Network Benchmark
(SNB)~\cite{Erling:2015:LSN:2723372.2742786} provided by LDBC represents three
distinct benchmarks. The network nodes correspond to people and the edges
represent their friendship and messages they post in discussion trees on their
forums. The three query workloads involve: (1) SNB-Interactive, i.e., complex
read-only queries accessing a high portion of data, (2) SNB-BI, i.e.,
queries accessing a high percentage of  entities and grouping them in various
dimensions, and (3) SNB-Algorithms, involving graph analysis algorithms, such as
community detection, PageRank, BFS, and clustering. The graph generator, called
Datagen, is a fork of S3G2 ~\cite{Pham2013} and realizes power laws, uses skewed
value distributions, and ensures reasonable correlations between graph
structures and property values. Additionally, it extends S3G2 with "spiky"
patterns in the distribution of social network activity along the timeline, also
provides the ability of generating update streams to the social network. Datagen
is also based on Hadoop in order to provide scalability, but comparet to S3G2 it
contains numerous performance improvements and the ability to be deterministic
regardless of the number of computer nodes used for the generation of the graphs
and for a given set of configuration parameters.

%The generated data have become a part if several graph benchmarks, such as GraphBIG~\cite{Nai:2015:GUG:2807591.2807626}.

\paragraph{Towards More Realistic Data} \cite{Nettleton2016} argued that the majority of existing works focuses on
topology generation which approximates the features of a real-world social
network (e.g., community structures, skew degree
distribution, a small average path length, or a small graph diameter); however,  this is usually done without any data. Hence, they
introduced a general stochastic modeling approach that enables the users to
fill a graph topology with data. The approach has three steps: (1) topology
generation (using R-MAT) plus community identification using the Louvain
method~\cite{1742-5468-2008-10-P10008} or usage of a real-world topology from
SNAP\footnote{\url{https://snap.stanford.edu/data/}}, (2) data definition
that describes  definitions of attribute values (distribution profiles) using a
parameterizable set of affinities and data propagation rules, and (3) data
population.

\paragraph{Strengths and Weaknesses of Social Network Generators}
Compared to more general graph generators, social network generators focus
mainly on reproducing intra- and inter-node feature correlations.
Among existing generators, LDBC SNB and S3G2 look
like the most advanced ones in terms of the complexity of the generated graph
and the amount of features and correlations they can generate, while providing a
large degree of scalability. Their generation process is based on input dictionaries and have configuration files that
allow tweaking  many parameters of the generated graphs, but their schema is mainly
static and cannot be easily configured to meet the needs of other uses cases
besides the benchmarks they have been designed for. In this regard, the approaches
like those proposed in~\cite{Nettleton2016} and~\cite{Sukthankar-SocialInfo2014} offer a
more flexible and understandable configuration process to tweak the types,
values, and correlations between different features.

Regarding the correlation between the underlying graph structure and the node
features, approaches such as LDBC SNB, S3G2 or ~\cite{Nettleton2016}
take into account this aspect and the generated graphs have realistic
structural properties while similar nodes have a larger probability of being
connected. However, their approach seems to be more based on
intuition and common sense than to be backed up by  any study of how the
relation between structure and attributes showcase in real social networks. In
this regard, this remains as a clear open challenge for social network
generators.

Finally, scalability is another aspect to be considered in social network graph
generators. LDBC SNB and S3G2 are engineered with this in mind, thus they
provide a way to scale to billions of nodes and edges. This is not the case for
the other generators, which can make them impractical if our goal is to generate
real sized social network graphs.




%\input{generators_analytics}

\subsection{Testing Community Detection}
\label{sec:generators_community_detection}

Community detection is one of the many graph analytics algorithms typically used
in domains such as social networks or bioinformatics. \emph{Communities} are usually defined as sets of nodes that are highly mutually connected, while being scarcely connected to
the other nodes of the graph. Such communities emerge from the fact that real-world graphs
are not random, but follow real-world dynamics that make similar entities to
have a larger probability to be connected. As a consequence, detected
communities are used to reveal vertices with similar characteristics, for
instance to discover functionally equivalent proteins in protein-to-protein
interaction networks, or persons with similar interests in social networks. Such
applications have made community detection a hot topic during the last 15
years with tens of developed algorithms and detection
strategies~\cite{doi:10.1002/wics.1403,Kim:2015:CDM:2854006.2854013}. For
comparing the quality of the different proposed techniques, one needs graphs
with \emph{reference communities}, that is, communities known beforehand. Since
it is very difficult to have large real-world graphs with reference communities
(mainly because these would require a manual labeling), graphs for benchmarking
community detection algorithms are typically generated synthetically.

\paragraph{Danon et al.} The first attempts to compare community detection algorithms using synthetic
graphs proposed the use of random graphs composed by several Erd\"{o}s-R\'{e}nyi
subgraphs, connected more internally than externally~\cite{danon2005comparing}.
Each of these subgraphs has the same size and the same internal/external density
of edges. However, such graphs miss the realism observed in real-world graphs, where
communities are of different sizes and densities, thus several proposals exist
to overcome such an issue.

\paragraph{LFR} Lancichinetti, Fortunato
and Radicchi (hence LFR)~\cite{PhysRevE.78.046110} propose a class of benchmark
graphs for community detection where communities are of diverse sizes and
densities. The generated communities follow a power-law distribution whose parameters can be configured. The degree of the
nodes is also sampled from a power-law distribution. Additionally, the generator
introduces the concept of the ``mixing factor'', which consists of the percentage of
edges in the graph connecting nodes that belong to distinct communities. Such parameter
allows  the degree of modularity of the generated graph  to be tuned, thus
testing the robustness of the algorithms under different conditions. The
generation process is implemented as an optimization process starting with an empty graph and
progressively filling it with nodes and edges guided by the specified constraints.

\paragraph{LFR-Overlapping} Lancichinetti, Fortunato and
Radicchi~\cite{PhysRevE.80.016118} extended LFR to support the notion of
directed graphs and overlapping communities. Overlapping communities extend the
notion of communities by allowing the sharing of vertices, thus a vertex can
belong to more than one community. This extended generator allows controlling
the same parameters of LFR, as well as the amount of overlap of
the generated communities.

\paragraph{Stochastic Block Models} Another popular family of generators
widely used in the community detection field are the stochastic block
models~\cite{holland1983stochastic}. In such models, the community structure of
the graph is typically defined as an array of $n$ community or cluster sizes
and a density square matrix of size $n \times n$ containing the density of
intra-cluster edges (in the diagonal of the matrix) and the density of
inter-cluster edges. Then, a stochastic procedure is run to sample graphs
from such array and matrix, using the sizes to compute the possible edges
and the densities as probabilities of such edges to exist. The popularity of
these methods stem from its simplicity and scalability, which makes them
suitable for generating large graphs fast and in a distributed environments,
provided that the density matrix is sparse (as it happens in most of real-world
                                            world graphs). Moreover, given
that the generation process of such models is mathematically tractable, they
are typically used to analyze the limitations of algorithms for community detection
 such as those based on modularity
optimization~\cite{fortunato2007resolution} or based on
triads~\cite{prat2016put}. Extensions of such models exist, such as the
Mixed Membership Stochastic Block Model~\cite{airoldi2008mixed}, for
overlapping communities.

\paragraph{Strengths and Weaknesses of Community Detection Generators}
Besides synthetic graph generators, Yang and Leskovec~\cite{yang2015defining}
proposed the use of real-world graphs with explicit group annotations (e.g.,
forums in a social network, categories of products, etc.) to infer what they
call \emph{meta-communities}, and use them to evaluate overlapping community
detection algorithms. However, a recent study from Hric, Darst and
Fortunato~\cite{hric2014community} reveal a loose correspondence between
communities (the authors refer to them as \emph{structural communities}) and
meta-communities.  This result reveals that  algorithms working for structural
communities do not work well for finding meta-communities and vice versa,
suggesting significantly different underlying characteristics
between the two types of communities, which are yet to be
identified.

In this regard and to the best of our knowledge, there are no available
generators that can generate graphs with meta-communities for community detection
algorithm benchmarking. The closest one is the LDBC SNB data
generator which has been provided by the
generation of groups of users in the social network. Even though the generation
process does not specifically enforce the generation of groups
(meta-communities) for benchmarking community detection algorithms, the study~\cite{Prat-Perez:2014:CSS:2621934.2621942} reveals that these groups are more similar
to the real meta-communities than those structural communities generated by the
LFR benchmark.

The differences observed between structural and meta-communities reveal the need
of more accurate community definitions tight more specifically to the domain or
use case. Current community detection algorithms and graph generators for
community detection are stuck to the traditional (and vague) definition of
community, assuming that there exists a single algorithm that would fit all the
use cases. Thus, future work requires the study of domain-specific community
characteristics that can be used to generate graphs with a community structure
that accurately resembles that of specific use cases, and thus revealing which
are the best algorithms for each particular scenario.




% \subsection{Specific Types of Graphs}
% ...

% % Note: not in timeline or tables
% \paragraph{Heterogeneous Graphs} The Heterogeneous Graph Data Benchmark (GDB-H)~\cite{Gupta:2012:GLH:2741795.2741808} aims at \emph{heterogenous graph} data model, a mixed model graph structure that combines several existing generation techniques into a single benchmark. The idea is demonstrated using a drug discovery scenario whose schema involves 11 entity categories (e.g., genes, proteins, diseases, ...) and 3000 binary relationships (e.g., instanceOf, subclassOf, ...). The data is structured as a combination of $N$ overlapping named graphs $G_1, ... G_N$, where the overlap is accomplished by node sharing. A subset of the named graphs $G_1, ... G_k$ are hierarchical, i.e., they are structured as trees or DAGs. The remaining $N-k$ graphs are multigraphs which differ in terms of their network connectivity properties (some component graphs obey the power-law more strictly, some graphs have a larger skew in the distribution of edge labels, some graphs  are denser, some graphs may optionally have additional constraints regarding subgraph patterns). The user can specify the number of component graphs (8 to 100), the number of nodes (100,000 to 100,000,000), the number of node types (3 to 11), and the number of distinct edge labels (30 - 3000), and optionally also type ratios (the fraction of the component graphs having hierarchical, power-law, community or motif structure), node distribution (i.e. the relative size of graph components), edge density, overlapping etc.

% For the purpose of generating the heterogeneous graphs having heterogeneous, i.e. hierarchical, power-law, community-structured or purely random, structure the authors combine several existing approaches corresponding to the particular structural type~\cite{PhysRevLett.102.128701,doi:10.1080/10586458.2001.10504428}.


% \subsection{Object-Oriented and XML Databases (?)}

% see~\cite{Dominguez-Sal:2010:DDG:1946050.1946053}
